{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import albumentations\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from albumentations.core.composition import Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import cv2\n",
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n",
    "import timm\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "pytorch_lightning.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'C:\\Users\\Kaggle\\Leaf_Classification\\input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read csv and label mapping\n",
    "\n",
    "labels = pd.read_csv(os.path.join(base_dir,'train.csv'))\n",
    "\n",
    "with open(f'{base_dir}/label_num_to_disease_map.json') as f:\n",
    "    label_mapper = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (labels['label'].value_counts(normalize=True,ascending=False)).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.6149460204701593,\n",
       " 4: 0.12043744450156564,\n",
       " 2: 0.11151095948030097,\n",
       " 1: 0.10230406131700706,\n",
       " 0: 0.05080151423096696}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_names: List,\n",
    "        transforms: Compose,\n",
    "        labels: Optional[List[int]],\n",
    "        img_path: str = '',\n",
    "        mode: str = 'train',\n",
    "        labels_to_ohe: bool = False,\n",
    "        n_classes: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Image classification dataset.\n",
    "\n",
    "        Args:\n",
    "            df: dataframe with image id and bboxes\n",
    "            mode: train/val/test\n",
    "            img_path: path to images\n",
    "            transforms: albumentations\n",
    "        \"\"\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        self.img_path = img_path\n",
    "        self.image_names = image_names\n",
    "        if labels is not None:\n",
    "            if not labels_to_ohe:\n",
    "                self.labels = np.array(labels)\n",
    "            else:\n",
    "                self.labels = np.zeros((len(labels), n_classes))\n",
    "                self.labels[np.arange(len(labels)), np.array(labels)] = 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, np.array]:\n",
    "        image_path = self.img_path + self.image_names[idx]\n",
    "        image = cv2.imread(f'{image_path}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(image_path)\n",
    "        target = self.labels[idx]\n",
    "\n",
    "        img = self.transforms(image=image)['image']\n",
    "        sample = {'image_path': image_path, 'image': img, 'target': np.array(target).astype('int64')}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz = 512\n",
    "sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentations taken from: https://www.kaggle.com/khyeh0719/pytorch-efficientnet-baseline-train-amp-aug\n",
    "train_augs = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Transpose(p=0.5),\n",
    "            albumentations.HorizontalFlip(p=0.5),\n",
    "            albumentations.VerticalFlip(p=0.5),\n",
    "            albumentations.ShiftScaleRotate(p=0.5),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=0.5),\n",
    "            albumentations.Cutout(p=0.5), \n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "  \n",
    "        \n",
    "valid_augs = albumentations.Compose([\n",
    "            albumentations.CenterCrop(sz, sz, p=1.),\n",
    "            albumentations.Resize(sz, sz),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0)\n",
    "            ,ToTensorV2()], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class conf:\n",
    "    def __init__(self,size,prob,beta):\n",
    "        super().__init__()\n",
    "        self.cropsize = size\n",
    "        self.prob = prob\n",
    "        self.beta = beta\n",
    "        \n",
    "conf = conf(sz,1,5)\n",
    "conf.prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam,center=False,attcen=None):\n",
    "    if len(size) == 4:\n",
    "        W = size[2]\n",
    "        H = size[3]\n",
    "    elif len(size) == 3:\n",
    "        W = size[1]\n",
    "        H = size[2]\n",
    "    elif len(size) == 2:\n",
    "        W = size[0]\n",
    "        H = size[1]\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    if attcen is None:\n",
    "        # uniform\n",
    "        cx = 0\n",
    "        cy = 0\n",
    "        if W>0 and H>0:\n",
    "            cx = np.random.randint(W)\n",
    "            cy = np.random.randint(H)\n",
    "        if center:\n",
    "            cx = int(W/2)\n",
    "            cy = int(H/2)\n",
    "    else:\n",
    "        cx = attcen[0]\n",
    "        cy = attcen[1]\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def get_bbox(imgsize=(224,224),beta=1.0):\n",
    "\n",
    "    r = np.random.rand(1)\n",
    "    lam = np.random.beta(beta, beta)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(imgsize, lam)\n",
    "\n",
    "    return [bbx1,bby1,bbx2,bby2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "def get_spm(input,target,conf,model,classifier,classifier_weight,classifier_bias):\n",
    "\n",
    "    imgsize = (conf.cropsize,conf.cropsize)\n",
    "    bs = input.size(0)\n",
    "    with torch.no_grad():\n",
    "        output,fms,_,_,_,_ = model(input,target)\n",
    "        clsw = classifier\n",
    "        weight = classifier_weight\n",
    "        bias = classifier_bias\n",
    "        weight = weight.view(weight.size(0),weight.size(1),1,1)\n",
    "        #fms = F.relu(fms)   ### this was already relu'ed from the conv net output\n",
    "        poolfea = F.adaptive_avg_pool2d(fms,(1,1)).squeeze()\n",
    "        clslogit = F.softmax(clsw.forward(poolfea))\n",
    "        #print(\"logit shape\",clslogit.shape)\n",
    "        logitlist = []\n",
    "        for i in range(bs):\n",
    "            logitlist.append(clslogit[i,target[i]])\n",
    "        clslogit = torch.stack(logitlist)\n",
    "\n",
    "        out = F.conv2d(fms, weight, bias=bias)\n",
    "\n",
    "        outmaps = []\n",
    "        for i in range(bs):\n",
    "            evimap = out[i,target[i]]\n",
    "            outmaps.append(evimap)\n",
    "\n",
    "        outmaps = torch.stack(outmaps)\n",
    "        if imgsize is not None:\n",
    "            outmaps = outmaps.view(outmaps.size(0),1,outmaps.size(1),outmaps.size(2))\n",
    "            outmaps = F.interpolate(outmaps,imgsize,mode='bilinear',align_corners=False)\n",
    "\n",
    "        outmaps = outmaps.squeeze()\n",
    "\n",
    "        for i in range(bs):\n",
    "            outmaps[i] -= outmaps[i].min()\n",
    "            outmaps[i] /= outmaps[i].sum()\n",
    "        \n",
    "        del clsw,weight,bias,poolfea,out,evimap,logitlist,target,input\n",
    "        gc.collect()\n",
    "\n",
    "    \n",
    "    return outmaps,clslogit\n",
    "\n",
    "\n",
    "\n",
    "def snapmix(input,target,conf,model,classifier,classifier_weight,classifier_bias):\n",
    "\n",
    "    r = np.random.rand(1)\n",
    "    lam_a = torch.ones(input.size(0))\n",
    "    lam_b = 1 - lam_a\n",
    "    target_b = target.clone()\n",
    "\n",
    "    if r < conf.prob:\n",
    "        wfmaps,_ = get_spm(input,target,conf,model,classifier,classifier_weight,classifier_bias)\n",
    "        bs = input.size(0)\n",
    "        lam = np.random.beta(conf.beta, conf.beta)\n",
    "        lam1 = np.random.beta(conf.beta, conf.beta)\n",
    "        rand_index = torch.randperm(bs).cuda()\n",
    "        wfmaps_b = wfmaps[rand_index,:,:]\n",
    "        target_b = target[rand_index]\n",
    "\n",
    "        same_label = target == target_b\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
    "        bbx1_1, bby1_1, bbx2_1, bby2_1 = rand_bbox(input.size(), lam1)\n",
    "\n",
    "        area = (bby2-bby1)*(bbx2-bbx1)\n",
    "        area1 = (bby2_1-bby1_1)*(bbx2_1-bbx1_1)\n",
    "\n",
    "        if  area1 > 0 and  area>0:\n",
    "            ncont = input[rand_index, :, bbx1_1:bbx2_1, bby1_1:bby2_1].clone()\n",
    "            ncont = F.interpolate(ncont, size=(bbx2-bbx1,bby2-bby1), mode='bilinear', align_corners=True)\n",
    "            input[:, :, bbx1:bbx2, bby1:bby2] = ncont\n",
    "            lam_a = 1 - wfmaps[:,bbx1:bbx2,bby1:bby2].sum(2).sum(1)/(wfmaps.sum(2).sum(1)+1e-8)\n",
    "            lam_b = wfmaps_b[:,bbx1_1:bbx2_1,bby1_1:bby2_1].sum(2).sum(1)/(wfmaps_b.sum(2).sum(1)+1e-8)\n",
    "            tmp = lam_a.clone()\n",
    "            lam_a[same_label] += lam_b[same_label]\n",
    "            lam_b[same_label] += tmp[same_label]\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "            lam_a[torch.isnan(lam_a)] = lam\n",
    "            lam_b[torch.isnan(lam_b)] = 1-lam\n",
    "\n",
    "    return input,target,target_b,lam_a,lam_b\n",
    "\n",
    "\n",
    "def as_cutmix(input,target,conf,model=None):\n",
    "\n",
    "    r = np.random.rand(1)\n",
    "    lam_a = torch.ones(input.size(0))\n",
    "    lam_b = 1 - lam_a\n",
    "    target_b = target.clone()\n",
    "\n",
    "    if r < conf.prob:\n",
    "        bs = input.size(0)\n",
    "        lam = np.random.beta(conf.beta, conf.beta)\n",
    "        rand_index = torch.randperm(bs).cuda()\n",
    "        target_b = target[rand_index]\n",
    "\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
    "        bbx1_1, bby1_1, bbx2_1, bby2_1 = rand_bbox(input.size(), lam)\n",
    "\n",
    "        if (bby2_1-bby1_1)*(bbx2_1-bbx1_1) > 4 and  (bby2-bby1)*(bbx2-bbx1)>4:\n",
    "            ncont = input[rand_index, :, bbx1_1:bbx2_1, bby1_1:bby2_1].clone()\n",
    "            ncont = F.interpolate(ncont, size=(bbx2-bbx1,bby2-bby1), mode='bilinear', align_corners=True)\n",
    "            input[:, :, bbx1:bbx2, bby1:bby2] = ncont\n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            lam_a = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "            lam_a *= torch.ones(input.size(0))\n",
    "    lam_b = 1 - lam_a\n",
    "\n",
    "    return input,target,target_b,lam_a.cuda(),lam_b.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL Data Module - https://www.kaggle.com/artgor/cassava-disease-identification-with-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 train_augs,\n",
    "                 valid_augs,\n",
    "                 path,\n",
    "                bs=8,\n",
    "                fold=0):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.train_augs = train_augs\n",
    "        self.valid_augs = valid_augs\n",
    "        self.path = path\n",
    "        self.bs = bs\n",
    "        self.fold = fold\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        train_indexes, valid_indexes = list(folds.split(self.df, self.df['label']))[self.fold]\n",
    "        \n",
    "        train_df = self.df.iloc[train_indexes]\n",
    "        valid_df = self.df.iloc[valid_indexes]\n",
    "\n",
    "        \n",
    "        self.train_dataset = ImageClassificationDataset(image_names=train_df['image_id'].values,\n",
    "                                                        transforms=train_augs,\n",
    "                                                        labels=train_df['label'].values,\n",
    "                                                        img_path=self.path,\n",
    "                                                        mode='train',\n",
    "                                                        labels_to_ohe=False,\n",
    "                                                        n_classes=5)\n",
    "        self.valid_dataset = ImageClassificationDataset(image_names=valid_df['image_id'].values,\n",
    "                                                        transforms=valid_augs,\n",
    "                                                        labels=valid_df['label'].values,\n",
    "                                                        img_path=self.path,\n",
    "                                                        mode='valid',\n",
    "                                                        labels_to_ohe=False,\n",
    "                                                        n_classes=5)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.bs,\n",
    "            num_workers=0,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.bs,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        return valid_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Net - https://www.kaggle.com/artgor/cassava-disease-identification-with-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adv_inception_v3',\n",
       " 'cspdarknet53',\n",
       " 'cspdarknet53_iabn',\n",
       " 'cspresnet50',\n",
       " 'cspresnet50d',\n",
       " 'cspresnet50w',\n",
       " 'cspresnext50',\n",
       " 'cspresnext50_iabn',\n",
       " 'darknet53',\n",
       " 'densenet121',\n",
       " 'densenet121d',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'densenet264',\n",
       " 'densenet264d_iabn',\n",
       " 'densenetblur121d',\n",
       " 'dla34',\n",
       " 'dla46_c',\n",
       " 'dla46x_c',\n",
       " 'dla60',\n",
       " 'dla60_res2net',\n",
       " 'dla60_res2next',\n",
       " 'dla60x',\n",
       " 'dla60x_c',\n",
       " 'dla102',\n",
       " 'dla102x',\n",
       " 'dla102x2',\n",
       " 'dla169',\n",
       " 'dpn68',\n",
       " 'dpn68b',\n",
       " 'dpn92',\n",
       " 'dpn98',\n",
       " 'dpn107',\n",
       " 'dpn131',\n",
       " 'eca_vovnet39b',\n",
       " 'ecaresnet18',\n",
       " 'ecaresnet50',\n",
       " 'ecaresnet50d',\n",
       " 'ecaresnet50d_pruned',\n",
       " 'ecaresnet101d',\n",
       " 'ecaresnet101d_pruned',\n",
       " 'ecaresnetlight',\n",
       " 'ecaresnext26tn_32x4d',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b1_pruned',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b2_pruned',\n",
       " 'efficientnet_b2a',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b3_pruned',\n",
       " 'efficientnet_b3a',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_b8',\n",
       " 'efficientnet_cc_b0_4e',\n",
       " 'efficientnet_cc_b0_8e',\n",
       " 'efficientnet_cc_b1_8e',\n",
       " 'efficientnet_el',\n",
       " 'efficientnet_em',\n",
       " 'efficientnet_es',\n",
       " 'efficientnet_l2',\n",
       " 'efficientnet_lite0',\n",
       " 'efficientnet_lite1',\n",
       " 'efficientnet_lite2',\n",
       " 'efficientnet_lite3',\n",
       " 'efficientnet_lite4',\n",
       " 'ens_adv_inception_resnet_v2',\n",
       " 'ese_vovnet19b_dw',\n",
       " 'ese_vovnet19b_slim',\n",
       " 'ese_vovnet19b_slim_dw',\n",
       " 'ese_vovnet39b',\n",
       " 'ese_vovnet39b_evos',\n",
       " 'ese_vovnet57b',\n",
       " 'ese_vovnet99b',\n",
       " 'ese_vovnet99b_iabn',\n",
       " 'fbnetc_100',\n",
       " 'gluon_inception_v3',\n",
       " 'gluon_resnet18_v1b',\n",
       " 'gluon_resnet34_v1b',\n",
       " 'gluon_resnet50_v1b',\n",
       " 'gluon_resnet50_v1c',\n",
       " 'gluon_resnet50_v1d',\n",
       " 'gluon_resnet50_v1s',\n",
       " 'gluon_resnet101_v1b',\n",
       " 'gluon_resnet101_v1c',\n",
       " 'gluon_resnet101_v1d',\n",
       " 'gluon_resnet101_v1s',\n",
       " 'gluon_resnet152_v1b',\n",
       " 'gluon_resnet152_v1c',\n",
       " 'gluon_resnet152_v1d',\n",
       " 'gluon_resnet152_v1s',\n",
       " 'gluon_resnext50_32x4d',\n",
       " 'gluon_resnext101_32x4d',\n",
       " 'gluon_resnext101_64x4d',\n",
       " 'gluon_senet154',\n",
       " 'gluon_seresnext50_32x4d',\n",
       " 'gluon_seresnext101_32x4d',\n",
       " 'gluon_seresnext101_64x4d',\n",
       " 'gluon_xception65',\n",
       " 'hrnet_w18',\n",
       " 'hrnet_w18_small',\n",
       " 'hrnet_w18_small_v2',\n",
       " 'hrnet_w30',\n",
       " 'hrnet_w32',\n",
       " 'hrnet_w40',\n",
       " 'hrnet_w44',\n",
       " 'hrnet_w48',\n",
       " 'hrnet_w64',\n",
       " 'ig_resnext101_32x8d',\n",
       " 'ig_resnext101_32x16d',\n",
       " 'ig_resnext101_32x32d',\n",
       " 'ig_resnext101_32x48d',\n",
       " 'inception_resnet_v2',\n",
       " 'inception_v3',\n",
       " 'inception_v4',\n",
       " 'legacy_senet154',\n",
       " 'legacy_seresnet18',\n",
       " 'legacy_seresnet34',\n",
       " 'legacy_seresnet50',\n",
       " 'legacy_seresnet101',\n",
       " 'legacy_seresnet152',\n",
       " 'legacy_seresnext26_32x4d',\n",
       " 'legacy_seresnext50_32x4d',\n",
       " 'legacy_seresnext101_32x4d',\n",
       " 'mixnet_l',\n",
       " 'mixnet_m',\n",
       " 'mixnet_s',\n",
       " 'mixnet_xl',\n",
       " 'mixnet_xxl',\n",
       " 'mnasnet_050',\n",
       " 'mnasnet_075',\n",
       " 'mnasnet_100',\n",
       " 'mnasnet_140',\n",
       " 'mnasnet_a1',\n",
       " 'mnasnet_b1',\n",
       " 'mnasnet_small',\n",
       " 'mobilenetv2_100',\n",
       " 'mobilenetv2_110d',\n",
       " 'mobilenetv2_120d',\n",
       " 'mobilenetv2_140',\n",
       " 'mobilenetv3_large_075',\n",
       " 'mobilenetv3_large_100',\n",
       " 'mobilenetv3_rw',\n",
       " 'mobilenetv3_small_075',\n",
       " 'mobilenetv3_small_100',\n",
       " 'nasnetalarge',\n",
       " 'pnasnet5large',\n",
       " 'regnetx_002',\n",
       " 'regnetx_004',\n",
       " 'regnetx_006',\n",
       " 'regnetx_008',\n",
       " 'regnetx_016',\n",
       " 'regnetx_032',\n",
       " 'regnetx_040',\n",
       " 'regnetx_064',\n",
       " 'regnetx_080',\n",
       " 'regnetx_120',\n",
       " 'regnetx_160',\n",
       " 'regnetx_320',\n",
       " 'regnety_002',\n",
       " 'regnety_004',\n",
       " 'regnety_006',\n",
       " 'regnety_008',\n",
       " 'regnety_016',\n",
       " 'regnety_032',\n",
       " 'regnety_040',\n",
       " 'regnety_064',\n",
       " 'regnety_080',\n",
       " 'regnety_120',\n",
       " 'regnety_160',\n",
       " 'regnety_320',\n",
       " 'res2net50_14w_8s',\n",
       " 'res2net50_26w_4s',\n",
       " 'res2net50_26w_6s',\n",
       " 'res2net50_26w_8s',\n",
       " 'res2net50_48w_2s',\n",
       " 'res2net101_26w_4s',\n",
       " 'res2next50',\n",
       " 'resnest14d',\n",
       " 'resnest26d',\n",
       " 'resnest50d',\n",
       " 'resnest50d_1s4x24d',\n",
       " 'resnest50d_4s2x40d',\n",
       " 'resnest101e',\n",
       " 'resnest200e',\n",
       " 'resnest269e',\n",
       " 'resnet18',\n",
       " 'resnet18d',\n",
       " 'resnet26',\n",
       " 'resnet26d',\n",
       " 'resnet34',\n",
       " 'resnet34d',\n",
       " 'resnet50',\n",
       " 'resnet50d',\n",
       " 'resnet66d',\n",
       " 'resnet101',\n",
       " 'resnet101d',\n",
       " 'resnet152',\n",
       " 'resnet152d',\n",
       " 'resnet200',\n",
       " 'resnet200d',\n",
       " 'resnetblur18',\n",
       " 'resnetblur50',\n",
       " 'resnext50_32x4d',\n",
       " 'resnext50d_32x4d',\n",
       " 'resnext101_32x4d',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'rexnet_100',\n",
       " 'rexnet_130',\n",
       " 'rexnet_150',\n",
       " 'rexnet_200',\n",
       " 'rexnetr_100',\n",
       " 'rexnetr_130',\n",
       " 'rexnetr_150',\n",
       " 'rexnetr_200',\n",
       " 'selecsls42',\n",
       " 'selecsls42b',\n",
       " 'selecsls60',\n",
       " 'selecsls60b',\n",
       " 'selecsls84',\n",
       " 'semnasnet_050',\n",
       " 'semnasnet_075',\n",
       " 'semnasnet_100',\n",
       " 'semnasnet_140',\n",
       " 'senet154',\n",
       " 'seresnet18',\n",
       " 'seresnet34',\n",
       " 'seresnet50',\n",
       " 'seresnet50tn',\n",
       " 'seresnet101',\n",
       " 'seresnet152',\n",
       " 'seresnext26_32x4d',\n",
       " 'seresnext26d_32x4d',\n",
       " 'seresnext26t_32x4d',\n",
       " 'seresnext26tn_32x4d',\n",
       " 'seresnext50_32x4d',\n",
       " 'seresnext101_32x4d',\n",
       " 'seresnext101_32x8d',\n",
       " 'skresnet18',\n",
       " 'skresnet34',\n",
       " 'skresnet50',\n",
       " 'skresnet50d',\n",
       " 'skresnext50_32x4d',\n",
       " 'spnasnet_100',\n",
       " 'ssl_resnet18',\n",
       " 'ssl_resnet50',\n",
       " 'ssl_resnext50_32x4d',\n",
       " 'ssl_resnext101_32x4d',\n",
       " 'ssl_resnext101_32x8d',\n",
       " 'ssl_resnext101_32x16d',\n",
       " 'swsl_resnet18',\n",
       " 'swsl_resnet50',\n",
       " 'swsl_resnext50_32x4d',\n",
       " 'swsl_resnext101_32x4d',\n",
       " 'swsl_resnext101_32x8d',\n",
       " 'swsl_resnext101_32x16d',\n",
       " 'tf_efficientnet_b0',\n",
       " 'tf_efficientnet_b0_ap',\n",
       " 'tf_efficientnet_b0_ns',\n",
       " 'tf_efficientnet_b1',\n",
       " 'tf_efficientnet_b1_ap',\n",
       " 'tf_efficientnet_b1_ns',\n",
       " 'tf_efficientnet_b2',\n",
       " 'tf_efficientnet_b2_ap',\n",
       " 'tf_efficientnet_b2_ns',\n",
       " 'tf_efficientnet_b3',\n",
       " 'tf_efficientnet_b3_ap',\n",
       " 'tf_efficientnet_b3_ns',\n",
       " 'tf_efficientnet_b4',\n",
       " 'tf_efficientnet_b4_ap',\n",
       " 'tf_efficientnet_b4_ns',\n",
       " 'tf_efficientnet_b5',\n",
       " 'tf_efficientnet_b5_ap',\n",
       " 'tf_efficientnet_b5_ns',\n",
       " 'tf_efficientnet_b6',\n",
       " 'tf_efficientnet_b6_ap',\n",
       " 'tf_efficientnet_b6_ns',\n",
       " 'tf_efficientnet_b7',\n",
       " 'tf_efficientnet_b7_ap',\n",
       " 'tf_efficientnet_b7_ns',\n",
       " 'tf_efficientnet_b8',\n",
       " 'tf_efficientnet_b8_ap',\n",
       " 'tf_efficientnet_cc_b0_4e',\n",
       " 'tf_efficientnet_cc_b0_8e',\n",
       " 'tf_efficientnet_cc_b1_8e',\n",
       " 'tf_efficientnet_el',\n",
       " 'tf_efficientnet_em',\n",
       " 'tf_efficientnet_es',\n",
       " 'tf_efficientnet_l2_ns',\n",
       " 'tf_efficientnet_l2_ns_475',\n",
       " 'tf_efficientnet_lite0',\n",
       " 'tf_efficientnet_lite1',\n",
       " 'tf_efficientnet_lite2',\n",
       " 'tf_efficientnet_lite3',\n",
       " 'tf_efficientnet_lite4',\n",
       " 'tf_inception_v3',\n",
       " 'tf_mixnet_l',\n",
       " 'tf_mixnet_m',\n",
       " 'tf_mixnet_s',\n",
       " 'tf_mobilenetv3_large_075',\n",
       " 'tf_mobilenetv3_large_100',\n",
       " 'tf_mobilenetv3_large_minimal_100',\n",
       " 'tf_mobilenetv3_small_075',\n",
       " 'tf_mobilenetv3_small_100',\n",
       " 'tf_mobilenetv3_small_minimal_100',\n",
       " 'tresnet_l',\n",
       " 'tresnet_l_448',\n",
       " 'tresnet_m',\n",
       " 'tresnet_m_448',\n",
       " 'tresnet_xl',\n",
       " 'tresnet_xl_448',\n",
       " 'tv_densenet121',\n",
       " 'tv_resnet34',\n",
       " 'tv_resnet50',\n",
       " 'tv_resnet101',\n",
       " 'tv_resnet152',\n",
       " 'tv_resnext50_32x4d',\n",
       " 'vit_base_patch16_224',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_huge_patch16_224',\n",
       " 'vit_huge_patch32_384',\n",
       " 'vit_large_patch16_224',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_patch32_384',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_resnet26d_224',\n",
       " 'vit_small_resnet50d_s3_224',\n",
       " 'vovnet39a',\n",
       " 'vovnet57a',\n",
       " 'wide_resnet50_2',\n",
       " 'wide_resnet101_2',\n",
       " 'xception',\n",
       " 'xception41',\n",
       " 'xception65',\n",
       " 'xception71']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_upd(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = timm.create_model('resnet50', pretrained=True)\n",
    "        output_dimension = list(self.net.children())[-1].in_features\n",
    "        self.net.fc = nn.Linear(output_dimension , 5)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        lastConv = nn.Sequential(*list(self.net.children())[:-2])(x)\n",
    "#         print(\"last conv shape\",lastConv.shape)\n",
    "        logits = self.net(x)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits,lastConv,loss,self.net.fc,self.net.fc.weight.data.detach(),self.net.fc.bias.data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCassava(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super(LitCassava, self).__init__()\n",
    "        self.model = model\n",
    "        self.metric = pl.metrics.Accuracy()\n",
    "        self.learning_rate = 1e-4\n",
    "\n",
    "    def forward(self, x, targets, *args, **kwargs):\n",
    "        return self.model(x, targets)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)\n",
    "        #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0 = 100,T_mult=2,eta_min=1e-6)\n",
    "\n",
    "        return (\n",
    "            [optimizer],\n",
    "            [{'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'valid_loss'}],\n",
    "        )\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: torch.Tensor, batch_idx: int\n",
    "    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n",
    "        image = batch['image']\n",
    "        target = batch['target']\n",
    "        logits,lastConv,loss,classifier,classifier_weight,classifier_bias = self(image, target)\n",
    "        \n",
    "#         print(\"weight\",classifier_weight)\n",
    "#         print(\"--------------------------------------------------------------------------------------\")\n",
    "#         print(\"bias\",classifier_bias)\n",
    "#         print(\"--------------------------------------------------------------------------------------\")\n",
    "        ### snapmix\n",
    "        const = np.random.randint(10)\n",
    "        if const<3:\n",
    "            # snapmix\n",
    "            #print(\"Snapmix envoked\")\n",
    "            mixed_input,target_a,target_b,lam_a,lam_b = snapmix(image,target,conf,model=self.model,\n",
    "                                                                classifier=classifier,classifier_weight=classifier_weight,\n",
    "                                                                classifier_bias=classifier_bias)\n",
    "            _,_,loss_a,_,_,_ = self(mixed_input,target_a)\n",
    "            _,_,loss_b,_,_,_ = self(mixed_input,target_b)\n",
    "            loss = torch.mean(loss_a* lam_a + loss_b* lam_b)\n",
    "            del mixed_input,target_a,target_b,lam_a,lam_b\n",
    "            \n",
    "        try:\n",
    "            loss = loss[0]\n",
    "            #rint(\"Is list\",loss)\n",
    "        except:\n",
    "            pass\n",
    "            #print(\"Not list\",loss)\n",
    "            #print(\"Snapmix loss\")\n",
    "        \n",
    "        score = self.metric(logits.argmax(1), target)\n",
    "        \n",
    "        del lastConv,classifier_weight,classifier_bias\n",
    "        gc.collect()\n",
    "        \n",
    "        logs = {'train_loss': loss, f'train_accuracy': score}\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'log': logs,\n",
    "            'progress_bar': logs,\n",
    "            'logits': logits,\n",
    "            'target': target,\n",
    "            f'train_accuracy': score,\n",
    "        }\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        #rint(\"Output shape:\",outputs)\n",
    "        #print([x['loss'] for x in outputs])\n",
    "        _ = ([x['loss'] for x in outputs])\n",
    "        avg_loss = torch.stack(_).mean()\n",
    "        del _; gc.collect()\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        y_true = torch.cat([x['target'] for x in outputs])\n",
    "        y_pred = torch.cat([x['logits'] for x in outputs])\n",
    "        score = self.metric(y_pred.argmax(1), y_true)\n",
    "        \n",
    "        logs = {'train_loss': avg_loss, 'train_accuracy': score}\n",
    "        return {'log': logs, 'progress_bar': logs}\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: torch.Tensor, batch_idx: int\n",
    "    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n",
    "        image = batch['image']\n",
    "        target = batch['target']\n",
    "        logits,lastConv, loss,_,_,_ = self(image, target)\n",
    "        score = self.metric(logits.argmax(1), target)\n",
    "        logs = {'valid_loss': loss, f'valid_accuracy': score}\n",
    "        \n",
    "        if isinstance(loss, list):\n",
    "            #print(\"Here\",loss)\n",
    "            loss = loss[0]\n",
    "                \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'log': logs,\n",
    "            'progress_bar': logs,\n",
    "            'logits': logits,\n",
    "            'target': target,\n",
    "            f'valid_accuracy': score,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        _ = ([x['loss'] for x in outputs])\n",
    "        avg_loss = torch.stack(_).mean()\n",
    "        del _; gc.collect()\n",
    "        #avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        y_true = torch.cat([x['target'] for x in outputs])\n",
    "        y_pred = torch.cat([x['logits'] for x in outputs])\n",
    "        score = self.metric(y_pred.argmax(1), y_true)\n",
    "\n",
    "        # score = torch.tensor(1.0, device=self.device)\n",
    "        logs = {'valid_loss': avg_loss, f'valid_accuracy': score, 'accuracy': score}\n",
    "        return {'valid_loss': avg_loss, 'log': logs, 'progress_bar': logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type     | Params\n",
      "------------------------------------\n",
      "0 | model  | Net_upd  | 23.5 M\n",
      "1 | metric | Accuracy | 0     \n",
      "------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fba96e1b5b4450a9d602d3fb0e7449d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:49: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:49: UserWarning: The {progress_bar:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for f in range(5):\n",
    "    print(f\"Running Fold:{f}\")\n",
    "    model = Net_upd()\n",
    "    dm = CassavaDataModule(labels, train_augs, valid_augs, f'{base_dir}/train_images/',bs=8,fold=f)\n",
    "    \n",
    "    modelSavePath = f'C:/Users/Kaggle/Leaf_Classification/saved_models/{f}'\n",
    "    if not os.path.exists(modelSavePath):\n",
    "        os.makedirs(modelSavePath)\n",
    "        \n",
    "    trainer = pl.Trainer(\n",
    "        checkpoint_callback=ModelCheckpoint(monitor='valid_accuracy',\n",
    "                                            save_top_k=2, dirpath = modelSavePath,filename='{epoch}_{valid_loss:.4f}_{valid_accuracy:.4f}', mode='min'),\n",
    "        accumulate_grad_batches = 1,\n",
    "        gpus=1,\n",
    "        max_epochs=20,\n",
    "        num_sanity_val_steps=0,\n",
    "        weights_summary='top',\n",
    "#         precision = 16,\n",
    "#         amp_backend = 'native',\n",
    "        callbacks = [EarlyStopping(monitor='valid_accuracy', patience=4, mode='min')]\n",
    "    )\n",
    "    \n",
    "    lit_model = LitCassava(model)\n",
    "                              \n",
    "    trainer.fit(lit_model, dm)\n",
    "\n",
    "    del trainer,model,lit_model\n",
    "    gc.collect()\n",
    "    time.sleep(60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
