{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import timm\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import albumentations\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from albumentations.core.composition import Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import cv2\n",
    "import torch\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "pytorch_lightning.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'C:\\Users\\Kaggle\\Leaf_Classification\\input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read csv and label mapping\n",
    "\n",
    "labels = pd.read_csv(os.path.join(base_dir,'train.csv'))\n",
    "\n",
    "with open(f'{base_dir}/label_num_to_disease_map.json') as f:\n",
    "    label_mapper = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_names: List,\n",
    "        transforms: Compose,\n",
    "        labels: Optional[List[int]],\n",
    "        img_path: str = '',\n",
    "        mode: str = 'train',\n",
    "        labels_to_ohe: bool = False,\n",
    "        n_classes: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Image classification dataset.\n",
    "\n",
    "        Args:\n",
    "            df: dataframe with image id and bboxes\n",
    "            mode: train/val/test\n",
    "            img_path: path to images\n",
    "            transforms: albumentations\n",
    "        \"\"\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        self.img_path = img_path\n",
    "        self.image_names = image_names\n",
    "        if labels is not None:\n",
    "            if not labels_to_ohe:\n",
    "                self.labels = np.array(labels)\n",
    "            else:\n",
    "                self.labels = np.zeros((len(labels), n_classes))\n",
    "                self.labels[np.arange(len(labels)), np.array(labels)] = 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, np.array]:\n",
    "        image_path = self.img_path + self.image_names[idx]\n",
    "        image = cv2.imread(f'{image_path}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(image_path)\n",
    "        target = self.labels[idx]\n",
    "\n",
    "        img = self.transforms(image=image)['image']\n",
    "        sample = {'image_path': image_path, 'image': img, 'target': np.array(target).astype('int64')}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 576\n",
    "sz_efffnetb4 = 600\n",
    "deit_size = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTA0_deit = albumentations.Compose([\n",
    "            albumentations.Resize(deit_size, deit_size,p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA1_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.HorizontalFlip(p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA2_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.VerticalFlip(p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA2_5_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.Transpose(p=1.),\n",
    "           albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA3_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=1\n",
    "            ),\n",
    "           albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA4_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=1.\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA5_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.Cutout(p=1.), \n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA6_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.ShiftScaleRotate(p=1.),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA7_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=1.),\n",
    "            ToTensorV2()],\n",
    "            p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTA0 = albumentations.Compose([\n",
    "            albumentations.Resize(sz, sz,p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA1 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.HorizontalFlip(p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA2 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.VerticalFlip(p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA2_5 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Transpose(p=1.),\n",
    "           albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA3 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=1\n",
    "            ),\n",
    "           albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA4 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=1.\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA5 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.Cutout(p=1.), \n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA6 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.ShiftScaleRotate(p=1.),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA7 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=1.),\n",
    "            ToTensorV2()],\n",
    "            p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_augs_effnet = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz_efffnetb4, sz_efffnetb4),\n",
    "            albumentations.Transpose(p=0.5),\n",
    "            albumentations.HorizontalFlip(p=0.5),\n",
    "            albumentations.VerticalFlip(p=0.5),\n",
    "            albumentations.ShiftScaleRotate(p=0.5),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=0.5),\n",
    "            albumentations.Cutout(p=0.5), \n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "infer_augs_resnext = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Transpose(p=0.5),\n",
    "            albumentations.HorizontalFlip(p=0.5),\n",
    "            albumentations.VerticalFlip(p=0.5),\n",
    "            albumentations.ShiftScaleRotate(p=0.5),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=0.5),\n",
    "            albumentations.Cutout(p=0.5), \n",
    "            ToTensorV2()],\n",
    "            p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_until(net: Any, param_name: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Freeze net until param_name\n",
    "\n",
    "\n",
    "    Args:\n",
    "        net:\n",
    "        param_name:\n",
    "\n",
    "    \"\"\"\n",
    "    found_name = False\n",
    "    for name, params in net.named_parameters():\n",
    "        if name == param_name:\n",
    "            found_name = True\n",
    "        params.requires_grad = found_name\n",
    "\n",
    "class BasicEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        arch: str = 'resnet18',\n",
    "        source: str = 'torchvision',\n",
    "        pretrained: str = None,\n",
    "        n_layers: int = -1,\n",
    "        freeze: bool = False,\n",
    "        to_one_channel: bool = False,\n",
    "        freeze_until_layer: str = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Encoder.\n",
    "\n",
    "        Args:\n",
    "            num_classes: the number of target classes, the size of the last layer's output\n",
    "            arch: the name of the architecture form pretrainedmodels\n",
    "            pretrained: the mode for pretrained model from pretrainedmodels\n",
    "            n_layers: number of layers to keep\n",
    "            freeze: to freeze model\n",
    "            freeze_until: freeze until this layer. If None, then freeze all layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if source == 'timm':\n",
    "            net = timm.create_model(arch, pretrained=pretrained)\n",
    "            self.output_dimension = list(net.children())[-1].in_features\n",
    "        else:\n",
    "            print(\"Not implemented\")\n",
    "            \n",
    "        if freeze:\n",
    "            freeze_until(net, freeze_until_layer)\n",
    "        \n",
    "        layers = list(net.children())[:n_layers]    \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDecoder(nn.Module):\n",
    "    def __init__(self, pool_output_size: int = 1, n_classes: int = 5, output_dimension: int = 512) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Decoder.\n",
    "\n",
    "        Args:\n",
    "            pool_output_size: the size of the result feature map after adaptive pooling layer\n",
    "            n_classes: n classes to output\n",
    "            output_dimension: output dimension of encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.pool = nn.AdaptiveAvgPool2d(output_size=pool_output_size)\n",
    "        self.fc = nn.Linear(output_dimension , n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.pool(x)\n",
    "        output = self.fc(x.view(x.size()[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDecoder_newhead(nn.Module):\n",
    "    def __init__(self, pool_output_size: int = 1, n_classes: int = 5, output_dimension: int = 512) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Decoder.\n",
    "\n",
    "        Args:\n",
    "            pool_output_size: the size of the result feature map after adaptive pooling layer\n",
    "            n_classes: n classes to output\n",
    "            output_dimension: output dimension of encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.pool = nn.AdaptiveAvgPool2d(output_size=pool_output_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(output_dimension, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.pool(x)\n",
    "        output = self.fc(x.view(x.size()[0], -1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_B4_newhead(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='tf_efficientnet_b4_ns',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_newhead(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_B3(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='tf_efficientnet_b3_ns',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_newhead(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_B4(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='tf_efficientnet_b4_ns',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_B5(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='tf_efficientnet_b5_ns',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDecoder_seresnext(nn.Module):\n",
    "    def __init__(self, pool_output_size: int = 1, n_classes: int = 5, output_dimension: int = 512) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Decoder.\n",
    "\n",
    "        Args:\n",
    "            pool_output_size: the size of the result feature map after adaptive pooling layer\n",
    "            n_classes: n classes to output\n",
    "            output_dimension: output dimension of encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.pool = nn.AdaptiveAvgPool2d(output_size=pool_output_size)\n",
    "        #self.fc = nn.Linear(output_dimension , n_classes)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(output_dimension, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 5),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #x = self.pool(x)\n",
    "        output = self.fc(x.view(x.size()[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seResnextNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='seresnext50_32x4d',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_seresnext(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnextNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='resnext50_32x4d',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnextNet50d(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='resnext50d_32x4d',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_newhead(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnest50d(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='resnest50d_1s4x24d',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_newhead(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEIT(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_384', pretrained=True)\n",
    "        output_dimension = list(self.encoder.children())[-1].in_features\n",
    "\n",
    "        self.encoder.head = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 5),\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    def forward(self, x, targets):\n",
    "        logits = self.encoder(x)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCassava(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super(LitCassava, self).__init__()\n",
    "        self.model = model\n",
    "        self.metric = pl.metrics.Accuracy()\n",
    "        self.learning_rate = 1e-4\n",
    "\n",
    "    def forward(self, x, targets, *args, **kwargs):\n",
    "        return self.model(x, targets)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)\n",
    "\n",
    "        return (\n",
    "            [optimizer],\n",
    "            [{'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'valid_loss'}],\n",
    "        )\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: torch.Tensor, batch_idx: int\n",
    "    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n",
    "        image = batch['image']\n",
    "        target = batch['target']\n",
    "        logits, loss = self(image, target)\n",
    "        score = self.metric(logits.argmax(1), target)\n",
    "        logs = {'train_loss': loss, f'train_accuracy': score}\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'log': logs,\n",
    "            'progress_bar': logs,\n",
    "            'logits': logits,\n",
    "            'target': target,\n",
    "            f'train_accuracy': score,\n",
    "        }\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        y_true = torch.cat([x['target'] for x in outputs])\n",
    "        y_pred = torch.cat([x['logits'] for x in outputs])\n",
    "        score = self.metric(y_pred.argmax(1), y_true)\n",
    "        \n",
    "        logs = {'train_loss': avg_loss, 'train_accuracy': score}\n",
    "        return {'log': logs, 'progress_bar': logs}\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: torch.Tensor, batch_idx: int\n",
    "    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n",
    "        image = batch['image']\n",
    "        target = batch['target']\n",
    "        logits, loss = self(image, target)\n",
    "        score = self.metric(logits.argmax(1), target)\n",
    "        logs = {'valid_loss': loss, f'valid_accuracy': score}\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'log': logs,\n",
    "            'progress_bar': logs,\n",
    "            'logits': logits,\n",
    "            'target': target,\n",
    "            f'valid_accuracy': score,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        y_true = torch.cat([x['target'] for x in outputs])\n",
    "        y_pred = torch.cat([x['logits'] for x in outputs])\n",
    "        score = self.metric(y_pred.argmax(1), y_true)\n",
    "\n",
    "        # score = torch.tensor(1.0, device=self.device)\n",
    "        logs = {'valid_loss': avg_loss, f'valid_accuracy': score, 'accuracy': score}\n",
    "        return {'valid_loss': avg_loss, 'log': logs, 'progress_bar': logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"../input/cassava-leaf-disease-classification/\"\n",
    "# sub = pd.read_csv(f'{path}/sample_submission.csv')\n",
    "# sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getloader(df,path,bs=8,fold=0,tta=0,deit=False):\n",
    "    \n",
    "    if not deit:\n",
    "        if tta == 0:\n",
    "            augs = TTA0\n",
    "        if tta == 1:\n",
    "            augs = TTA1\n",
    "        if tta == 2:\n",
    "            augs = TTA2\n",
    "        if tta == 3:\n",
    "            augs = TTA2_5\n",
    "        if tta == 4:\n",
    "            augs = TTA3\n",
    "        if tta == 5:\n",
    "            augs = TTA4\n",
    "        if tta == 6:\n",
    "            augs = TTA5\n",
    "        if tta == 7:\n",
    "            augs = TTA6\n",
    "        if tta == 8:\n",
    "            augs = TTA7\n",
    "    else:\n",
    "        if tta == 0:\n",
    "            augs = TTA0_deit\n",
    "        if tta == 1:\n",
    "            augs = TTA1_deit\n",
    "        if tta == 2:\n",
    "            augs = TTA2_deit\n",
    "        if tta == 3:\n",
    "            augs = TTA2_5_deit\n",
    "        if tta == 4:\n",
    "            augs = TTA3_deit\n",
    "        if tta == 5:\n",
    "            augs = TTA4_deit\n",
    "        if tta == 6:\n",
    "            augs = TTA5_deit\n",
    "        if tta == 7:\n",
    "            augs = TTA6_deit\n",
    "        if tta == 8:\n",
    "            augs = TTA7_deit\n",
    "    \n",
    "    print(f\"Using TTA: {tta} and DEIT:{deit}\")\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    train_indexes, valid_indexes = list(folds.split(df, df['label']))[fold]\n",
    "\n",
    "    train_df = df.iloc[train_indexes]\n",
    "    valid_df = df.iloc[valid_indexes]\n",
    "\n",
    "\n",
    "    train_dataset = ImageClassificationDataset(image_names=train_df['image_id'].values,\n",
    "                                                    transforms=augs,\n",
    "                                                    labels=train_df['label'].values,\n",
    "                                                    img_path=path,\n",
    "                                                    mode='train',\n",
    "                                                    labels_to_ohe=False,\n",
    "                                                    n_classes=5)\n",
    "    valid_dataset = ImageClassificationDataset(image_names=valid_df['image_id'].values,\n",
    "                                                    transforms=augs,\n",
    "                                                    labels=valid_df['label'].values,\n",
    "                                                    img_path=path,\n",
    "                                                    mode='valid',\n",
    "                                                    labels_to_ohe=False,\n",
    "                                                    n_classes=5)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=bs,num_workers=0,shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=bs,num_workers=0,shuffle=False)\n",
    "\n",
    "    return train_loader,valid_loader,valid_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = os.listdir('../input/leaves-tf-enet-b4-ns-576-finetune-temperedloss/')\n",
    "# modelsPath = [os.path.join('../input/leaves-tf-enet-b4-ns-576-finetune-temperedloss',i) for i in models]\n",
    "# modelsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def getPreds(test_loader,lit_model,logitsOut=True):\n",
    "    predictions = []\n",
    "    \n",
    "    for batch in tqdm(test_loader):\n",
    "        image = batch['image'].to('cuda')\n",
    "        target = batch['target'].to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = lit_model.model(image, target)[0]\n",
    "            _outputs = outputs.softmax(1).detach().cpu().numpy()\n",
    "            preds = outputs.argmax(1).detach().cpu().numpy()\n",
    "            if not logitsOut:\n",
    "                predictions.append(preds)\n",
    "            else:\n",
    "                #predictions[range(len(_outputs))] += _outputs\n",
    "                predictions.append(_outputs)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcf_balance(cost):\n",
    "    from ortools.graph import pywrapgraph\n",
    "    import math\n",
    "    size, nc = cost.shape\n",
    "    PeC = [48,106,103,602,139]\n",
    "    tmp = 0\n",
    "    ncc = {}\n",
    "    for cid, percent in enumerate(PeC[:-1]):\n",
    "        num = int(size*percent/sum(PeC))\n",
    "        tmp += num\n",
    "        ncc[cid] = num\n",
    "    ncc[4] = size-tmp\n",
    "    mcf = pywrapgraph.SimpleMinCostFlow()\n",
    "    m = cost*-1000000000\n",
    "    modulo = 0#size % nc\n",
    "    mcf.SetNodeSupply(nc+size, modulo) # ext\n",
    "    for j in range(nc):\n",
    "        mcf.SetNodeSupply(j + size, int(ncc[j]))\n",
    "        mcf.AddArcWithCapacityAndUnitCost(nc + size, j + size, 1, 0) # ext\n",
    "    for i in range(size):\n",
    "        mcf.SetNodeSupply(i, -1)\n",
    "        for j in range(nc):\n",
    "            mcf.AddArcWithCapacityAndUnitCost(j + size, i, 1, 1000*int(m[i][j]))\n",
    "    mcf.SolveMaxFlowWithMinCost()\n",
    "\n",
    "    answ = np.zeros(size, dtype=np.int32)\n",
    "    for i in range(mcf.NumArcs()):\n",
    "        if mcf.Flow(i) > 0 and mcf.Head(i) < size:\n",
    "            answ[mcf.Head(i)] = mcf.Tail(i) - size\n",
    "    return answ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for DEIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_4.ckpt'}\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_DEIT = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = DEIT()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta,deit=True)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_DEIT[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_DEIT = preds_DEIT.copy()\n",
    "preds_mean_DEIT[:,0]= preds_DEIT[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_DEIT[:,1]= preds_DEIT[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_DEIT[:,2]= preds_DEIT[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_DEIT[:,3]= preds_DEIT[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_DEIT[:,4]= preds_DEIT[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_DEIT,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_DEIT,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for effnet B3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelsDir = os.listdir(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\'')\n",
    "\n",
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'}\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelsDir = os.listdir(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\)\n",
    "\n",
    "# modelsPath = [os.path.join(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\best_ones',i) for i in models]\n",
    "# modelsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_effnet_b3 = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B3()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b3[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_effnetb3 = preds_effnet_b3.copy()\n",
    "preds_mean_effnetb3[:,0]= preds_effnet_b3[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_effnetb3[:,1]= preds_effnet_b3[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_effnetb3[:,2]= preds_effnet_b3[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_effnetb3[:,3]= preds_effnet_b3[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_effnetb3[:,4]= preds_effnet_b3[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_effnetb3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_effnet_b3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for seresnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelsDir = os.listdir(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\'')\n",
    "\n",
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelsDir = os.listdir(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\)\n",
    "\n",
    "# modelsPath = [os.path.join(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\best_ones',i) for i in models]\n",
    "# modelsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_seresnext = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):\n",
    "    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = seResnextNet()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=128,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_seresnext[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_seresnext = preds_seresnext.copy()\n",
    "preds_mean_seresnext[:,0]= preds_mean_seresnext[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_seresnext[:,1]= preds_mean_seresnext[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_seresnext[:,2]= preds_mean_seresnext[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_seresnext[:,3]= preds_mean_seresnext[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_seresnext[:,4]= preds_mean_seresnext[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_seresnext,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classOut = np.argmax(preds_seresnext,1)\n",
    "# classOut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_seresnext,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for Effnet b4 Taylor Loss - 2 SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_effnet_b4_taylor = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B4_newhead()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b4_taylor[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_effnetb4_taylor = preds_effnet_b4_taylor.copy()\n",
    "preds_mean_effnetb4_taylor[:,0]= preds_effnet_b4_taylor[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_effnetb4_taylor[:,1]= preds_effnet_b4_taylor[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_effnetb4_taylor[:,2]= preds_effnet_b4_taylor[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_effnetb4_taylor[:,3]= preds_effnet_b4_taylor[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_effnetb4_taylor[:,4]= preds_effnet_b4_taylor[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_effnetb4_taylor,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_effnet_b4_taylor,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect();torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for Effnet b4 Taylor Loss - 3 SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_effnet_b4_taylor_3SWA = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B4_newhead()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b4_taylor_3SWA[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_effnetb4_taylor_3SWA = preds_effnet_b4_taylor_3SWA.copy()\n",
    "preds_mean_effnetb4_taylor_3SWA[:,0]= preds_effnet_b4_taylor_3SWA[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_effnetb4_taylor_3SWA[:,1]= preds_effnet_b4_taylor_3SWA[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_effnetb4_taylor_3SWA[:,2]= preds_effnet_b4_taylor_3SWA[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_effnetb4_taylor_3SWA[:,3]= preds_effnet_b4_taylor_3SWA[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_effnetb4_taylor_3SWA[:,4]= preds_effnet_b4_taylor_3SWA[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_effnetb4_taylor_3SWA,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_effnet_b4_taylor_3SWA,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect();torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for B4 focal cosine loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for fold:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 0 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [02:14<00:00,  1.00s/it]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 1 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:30<00:00,  1.48it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 2 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:31<00:00,  1.46it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 3 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:32<00:00,  1.46it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 4 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:37<00:00,  1.37it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for fold:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 0 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [02:12<00:00,  1.01it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 1 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/134 [00:02<02:24,  1.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-ea7b909d1d42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_ix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_img_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0msoftmaxOut\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgetPreds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlit_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogitsOut\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mpreds_effnet_b4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_ix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmaxOut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-2c67cfb60f16>\u001b[0m in \u001b[0;36mgetPreds\u001b[1;34m(test_loader, lit_model, logitsOut)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0m_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlogitsOut\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds_effnet_b4 = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B4_newhead()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b4[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.320560e-01</td>\n",
       "      <td>9.197152e-02</td>\n",
       "      <td>8.439804e-02</td>\n",
       "      <td>0.039143</td>\n",
       "      <td>1.524311e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6.610613e-16</td>\n",
       "      <td>7.499553e-16</td>\n",
       "      <td>7.548833e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.541935e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.773748e-05</td>\n",
       "      <td>1.236225e-04</td>\n",
       "      <td>1.213046e-04</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>9.994154e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.923629e-05</td>\n",
       "      <td>9.997238e-01</td>\n",
       "      <td>5.725026e-05</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>6.636565e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.196785e-22</td>\n",
       "      <td>2.668238e-22</td>\n",
       "      <td>2.471380e-22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.373224e-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label  fold             0             1             2  \\\n",
       "0  1000015157.jpg      0     3  6.320560e-01  9.197152e-02  8.439804e-02   \n",
       "1  1000201771.jpg      3     2  6.610613e-16  7.499553e-16  7.548833e-16   \n",
       "2   100042118.jpg      1     2  7.773748e-05  1.236225e-04  1.213046e-04   \n",
       "3  1000723321.jpg      1     1  5.923629e-05  9.997238e-01  5.725026e-05   \n",
       "4  1000812911.jpg      3     2  2.196785e-22  2.668238e-22  2.471380e-22   \n",
       "\n",
       "          3             4  \n",
       "0  0.039143  1.524311e-01  \n",
       "1  1.000000  9.541935e-16  \n",
       "2  0.000262  9.994154e-01  \n",
       "3  0.000093  6.636565e-05  \n",
       "4  1.000000  3.373224e-22  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_effnet_b4 = pd.read_csv(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\OOFs\\OOF_effnetb4_5TTA_576.csv')\n",
    "preds_DEIT = pd.read_csv(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\OOFs\\OOF_seresnext_5TTA_576.csv')\n",
    "preds_seresnext = pd.read_csv(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\OOFs\\OOF_seresnext_5TTA_576.csv')\n",
    "preds_effnet_b4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ortools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcf_balance(cost):\n",
    "    from ortools.graph import pywrapgraph\n",
    "    import math\n",
    "    size, nc = cost.shape\n",
    "    PeC = [48,106,103,602,139]\n",
    "    tmp = 0\n",
    "    ncc = {}\n",
    "    for cid, percent in enumerate(PeC[:-1]):\n",
    "        num = int(size*percent/sum(PeC))\n",
    "        tmp += num\n",
    "        ncc[cid] = num\n",
    "    ncc[4] = size-tmp\n",
    "    mcf = pywrapgraph.SimpleMinCostFlow()\n",
    "    m = cost*-1000000000\n",
    "    modulo = 0#size % nc\n",
    "    mcf.SetNodeSupply(nc+size, modulo) # ext\n",
    "    for j in range(nc):\n",
    "        mcf.SetNodeSupply(j + size, int(ncc[j]))\n",
    "        mcf.AddArcWithCapacityAndUnitCost(nc + size, j + size, 1, 0) # ext\n",
    "    for i in range(size):\n",
    "        mcf.SetNodeSupply(i, -1)\n",
    "        for j in range(nc):\n",
    "            mcf.AddArcWithCapacityAndUnitCost(j + size, i, 1, 1000*int(m[i][j]))\n",
    "    mcf.SolveMaxFlowWithMinCost()\n",
    "\n",
    "    answ = np.zeros(size, dtype=np.int32)\n",
    "    for i in range(mcf.NumArcs()):\n",
    "        if mcf.Flow(i) > 0 and mcf.Head(i) < size:\n",
    "            answ[mcf.Head(i)] = mcf.Tail(i) - size\n",
    "    return answ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.69209336e-01, 1.73036586e-01, 1.53435147e-01, 1.31330741e-01,\n",
       "        1.72988195e-01],\n",
       "       [5.74758778e-06, 6.10528710e-06, 5.83895139e-06, 9.99976369e-01,\n",
       "        5.90499780e-06],\n",
       "       [8.64700948e-02, 8.94704413e-02, 8.43327058e-02, 9.23392266e-02,\n",
       "        6.47387533e-01],\n",
       "       ...,\n",
       "       [1.19031994e-02, 9.50216744e-01, 1.22598821e-02, 1.27887759e-02,\n",
       "        1.28314170e-02],\n",
       "       [1.60181634e-01, 1.03817756e-01, 1.02472466e-01, 1.10471368e-01,\n",
       "        5.23056749e-01],\n",
       "       [8.10210852e-02, 8.32659213e-02, 6.49953438e-01, 8.90389635e-02,\n",
       "        9.67205622e-02]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_avg = (0.29*preds_effnet_b4.iloc[:,3:].values+0.25*preds_DEIT.iloc[:,3:].values+0.46*preds_seresnext.iloc[:,3:].values)\n",
    "preds_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99999997, 1.        , ..., 1.00000002, 0.99999997,\n",
       "       0.99999997])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_avg.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "classOut = mcf_balance(preds_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 4, ..., 1, 4, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8908258167032762"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut)#,accuracy_score(labels.label,np.argmax(preds_effnet_b4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect();torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble b4 with deit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = pd.DataFrame()\n",
    "res_['b4_contrib'] = 0\n",
    "#res['b5_contrib'] = 0\n",
    "res_['deit_contrib'] = 0\n",
    "res_ ['Accuracy'] = 0\n",
    "\n",
    "count = 0\n",
    "for i in np.arange(0,1,0.01):\n",
    "    b4_contrib = i\n",
    "    deit_contrib = 1-i\n",
    "    #print(i,j,1-i-j)\n",
    "    ens_preds = (b4_contrib*preds_effnet_b4+deit_contrib*preds_DEIT)\n",
    "    res_.loc[count,'b4_contrib'] = i\n",
    "    res_.loc[count,'deit_contrib'] = 1-i\n",
    "    res_.loc[count,'Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds,1))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = pd.DataFrame()\n",
    "res_['b4_contrib'] = 0\n",
    "#res['b5_contrib'] = 0\n",
    "res_['seresnext_contrib'] = 0\n",
    "res_ ['Accuracy'] = 0\n",
    "\n",
    "count = 0\n",
    "for i in np.arange(0,1,0.01):\n",
    "    b4_contrib = i\n",
    "    seresnext_contrib = 1-i\n",
    "    #print(i,j,1-i-j)\n",
    "    ens_preds = (b4_contrib*preds_effnet_b4+seresnext_contrib*preds_seresnext)\n",
    "    res_.loc[count,'b4_contrib'] = i\n",
    "    res_.loc[count,'seresnext_contrib'] = 1-i\n",
    "    res_.loc[count,'Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds,1))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble deit with b4 taylor loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = pd.DataFrame()\n",
    "res_['b4_taylor_contrib'] = 0\n",
    "#res['b5_contrib'] = 0\n",
    "res_['deit_contrib'] = 0\n",
    "res_ ['Accuracy'] = 0\n",
    "\n",
    "count = 0\n",
    "for i in np.arange(0,1,0.01):\n",
    "    b4_contrib = i\n",
    "    deit_contrib = 1-i\n",
    "    #print(i,j,1-i-j)\n",
    "    ens_preds = (b4_contrib*preds_effnet_b4_taylor+deit_contrib*preds_DEIT)\n",
    "    res_.loc[count,'b4_taylor_contrib'] = i\n",
    "    res_.loc[count,'deit_contrib'] = 1-i\n",
    "    res_.loc[count,'Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds,1))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble seresnext with b4 taylor loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = pd.DataFrame()\n",
    "res_['b4_taylor_contrib'] = 0\n",
    "#res['b5_contrib'] = 0\n",
    "res_['seresnext_contrib'] = 0\n",
    "res_ ['Accuracy'] = 0\n",
    "\n",
    "count = 0\n",
    "for i in np.arange(0,1,0.01):\n",
    "    b4_contrib = i\n",
    "    seresnext_contrib = 1-i\n",
    "    #print(i,j,1-i-j)\n",
    "    ens_preds = (b4_contrib*preds_effnet_b4_taylor+seresnext_contrib*preds_seresnext)\n",
    "    res_.loc[count,'b4_taylor_contrib'] = i\n",
    "    res_.loc[count,'seresnext_contrib'] = 1-i\n",
    "    res_.loc[count,'Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds,1))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble deit+seresnext+b4 focal cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res['b4_contrib'] = 0\n",
    "res['deit_contrib'] = 0\n",
    "res['seresnext_contrib'] = 0\n",
    "res ['Accuracy'] = 0\n",
    "res ['Adj_Accuracy'] = 0\n",
    "\n",
    "count = 0\n",
    "for i in np.arange(0,1,0.01):\n",
    "    for j in np.arange(0,1-i,0.01):\n",
    "        b4_contrib = i\n",
    "        deit_contrib = j\n",
    "        seresnext_contrib = 1-i-j\n",
    "        #print(i,j,1-i-j)\n",
    "        ens_preds_adj = (b4_contrib*preds_mean_effnetb4+seresnext_contrib*preds_mean_seresnext+deit_contrib*preds_mean_DEIT)\n",
    "        ens_preds = (b4_contrib*preds_effnet_b4+seresnext_contrib*preds_seresnext+deit_contrib*preds_DEIT)\n",
    "        res.loc[count,'b4_contrib'] = i\n",
    "        res.loc[count,'seresnext_contrib'] = 1-i-j\n",
    "        res.loc[count,'deit_contrib'] = j\n",
    "        res.loc[count,'Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds,1))\n",
    "        res.loc[count,'Adj_Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds_adj,1))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values(by=['Adj_Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for B5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_effnet_b5 = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):\n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B5()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b5[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_effnetb5 = preds_effnet_b5.copy()\n",
    "preds_mean_effnetb5[:,0]= preds_effnet_b5[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_effnetb5[:,1]= preds_effnet_b5[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_effnetb5[:,2]= preds_effnet_b5[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_effnetb5[:,3]= preds_effnet_b5[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_effnetb5[:,4]= preds_effnet_b5[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_effnetb5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_effnet_b5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect();torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res['b4_contrib'] = 0\n",
    "res['b5_contrib'] = 0\n",
    "res['seresnext_contrib'] = 0\n",
    "res ['Accuracy'] = 0\n",
    "res ['Adj_Accuracy'] = 0\n",
    "\n",
    "count = 0\n",
    "for i in np.arange(0,1,0.01):\n",
    "    for j in np.arange(0,1-i,0.01):\n",
    "        b4_contrib = i\n",
    "        b5_contrib = j\n",
    "        seresnext_contrib = 1-i-j\n",
    "        #print(i,j,1-i-j)\n",
    "        ens_preds_adj = (b4_contrib*preds_mean_effnetb4+seresnext_contrib*preds_mean_seresnext+b5_contrib*preds_mean_effnetb5)\n",
    "        ens_preds = (b4_contrib*preds_effnet_b4+seresnext_contrib*preds_seresnext+b5_contrib*preds_effnet_b5)\n",
    "        res.loc[count,'b4_contrib'] = i\n",
    "        res.loc[count,'seresnext_contrib'] = 1-i-j\n",
    "        res.loc[count,'b5_contrib'] = j\n",
    "        res.loc[count,'Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds,1))\n",
    "        res.loc[count,'Adj_Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds_adj,1))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values(by=['Adj_Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resnest50d_1s4x24d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_resnest50d = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):\n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = resnest50d()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_resnest50d[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_resnest50d = preds_resnest50d.copy()\n",
    "preds_mean_resnest50d[:,0]= preds_resnest50d[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_resnest50d[:,1]= preds_resnest50d[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_resnest50d[:,2]= preds_resnest50d[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_resnest50d[:,3]= preds_resnest50d[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_resnest50d[:,4]= preds_resnest50d[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_resnest50d,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_resnest50d,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnext50D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_resnext50d = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):\n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = ResnextNet50d()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "#     lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.load_state_dict(checkpoint)  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_resnext50d[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_resnext50d = preds_resnext50d.copy()\n",
    "preds_mean_resnext50d[:,0]= preds_resnext50d[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_resnext50d[:,1]= preds_resnext50d[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_resnext50d[:,2]= preds_resnext50d[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_resnext50d[:,3]= preds_resnext50d[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_resnext50d[:,4]= preds_resnext50d[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_resnext50d,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_resnest50d,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all4 = pd.DataFrame()\n",
    "res_all4['b4_contrib'] = 0\n",
    "res_all4['b5_contrib'] = 0\n",
    "res_all4['seresnext_contrib'] = 0\n",
    "res_all4['resnest_contrib'] = 0\n",
    "\n",
    "res_all4 ['Accuracy'] = 0\n",
    "res_all4 ['Adj_Accuracy'] = 0\n",
    "\n",
    "count = 0\n",
    "for i in np.arange(0,1,0.05):\n",
    "    for j in np.arange(0,1-i,0.05):\n",
    "        for k in np.arange(0,1-i-j,0.05):\n",
    "            b4_contrib = i\n",
    "            b5_contrib = j\n",
    "            seresnext_contrib = k\n",
    "            resnest_contrib = 1-i-j-k\n",
    "            print(b4_contrib+b5_contrib+seresnext_contrib+resnest_contrib)\n",
    "            #assert b4_contrib+b5_contrib+seresnext_contrib+resnest_contrib==1.\n",
    "            #print(i,j,k,1-(i+j+k))\n",
    "            ens_preds_adj = (b4_contrib*preds_mean_effnetb4+seresnext_contrib*preds_mean_seresnext+b5_contrib*preds_mean_effnetb5+resnest_contrib*preds_mean_resnest50d)\n",
    "            ens_preds = (b4_contrib*preds_effnet_b4+seresnext_contrib*preds_seresnext+b5_contrib*preds_effnet_b5+resnest_contrib*preds_resnest50d)\n",
    "            res_all4.loc[count,'b4_contrib'] = i\n",
    "            res_all4.loc[count,'seresnext_contrib'] = j\n",
    "            res_all4.loc[count,'b5_contrib'] = k\n",
    "            res_all4.loc[count,'resnest_contrib'] = 1-i-j-k\n",
    "            res_all4.loc[count,'Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds,1))\n",
    "            res_all4.loc[count,'Adj_Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds_adj,1))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all4.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all4.sort_values(by=['Adj_Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_DEIT\n",
    "#preds_mean_DEIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_all4 = pd.DataFrame()\n",
    "res_all4['DEIT_contrib'] = 0\n",
    "res_all4['b5_contrib'] = 0\n",
    "res_all4['seresnext_contrib'] = 0\n",
    "res_all4['resnest_contrib'] = 0\n",
    "\n",
    "res_all4 ['Accuracy'] = 0\n",
    "res_all4 ['Adj_Accuracy'] = 0\n",
    "\n",
    "count = 0\n",
    "for i in np.arange(0,1,0.05):\n",
    "    for j in np.arange(0,1-i,0.05):\n",
    "        for k in np.arange(0,1-i-j,0.05):\n",
    "            DEIT_contrib = i\n",
    "            b5_contrib = j \n",
    "            seresnext_contrib = k = 0 \n",
    "            resnest_contrib = 1-i-j-k\n",
    "            print(DEIT_contrib+b5_contrib+seresnext_contrib+resnest_contrib)\n",
    "            #assert b4_contrib+b5_contrib+seresnext_contrib+resnest_contrib==1.\n",
    "            #print(i,j,k,1-(i+j+k))\n",
    "            ens_preds_adj = (DEIT_contrib*preds_mean_DEIT+seresnext_contrib*preds_mean_seresnext+b5_contrib*preds_mean_effnetb5+resnest_contrib*preds_mean_resnest50d)\n",
    "            ens_preds = (DEIT_contrib*preds_DEIT+seresnext_contrib*preds_seresnext+b5_contrib*preds_effnet_b5+resnest_contrib*preds_resnest50d)\n",
    "            res_all4.loc[count,'DEIT_contrib'] = i\n",
    "            res_all4.loc[count,'seresnext_contrib'] = j\n",
    "            res_all4.loc[count,'b5_contrib'] = k\n",
    "            res_all4.loc[count,'resnest_contrib'] = 1-i-j-k\n",
    "            res_all4.loc[count,'Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds,1))\n",
    "            res_all4.loc[count,'Adj_Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds_adj,1))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.max(preds_DEIT,1),np.max(preds_effnet_b4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.max(preds_DEIT,1),np.max(preds_effnet_b5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.max(preds_DEIT,1),np.max(preds_seresnext,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.max(preds_DEIT,1),np.max(preds_resnest50d,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.max(preds_seresnext,1),np.max(preds_resnest50d,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.max(preds_effnet_b4_taylor,1),np.max(preds_effnet_b4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all4.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(labels.label,np.argmax(preds_effnet_b4,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels.label,np.argmax(preds_mean_effnetb4,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels.label,np.argmax(preds_effnet_b4_taylor,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels.label,np.argmax(preds_mean_effnetb4_taylor,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels.label,np.argmax(preds_DEIT,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels.label,np.argmax(preds_effnet_b5,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels.label,np.argmax(preds_resnest50d,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels.label,np.argmax(preds_resnext50d,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels.label,np.argmax(preds_seresnext,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4_contrib = 0.18\n",
    "seresnext_contrib = 0.61\n",
    "b5_contrib = 0.21\n",
    "_ = (b4_contrib*preds_effnet_b4+seresnext_contrib*preds_seresnext+b5_contrib*preds_effnet_b5)\n",
    "print(accuracy_score(labels.label,np.argmax(_,1)))\n",
    "confusion_matrix(labels.label,np.argmax(_,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deit_contrib = 0.15\n",
    "seresnext_contrib = 0.25\n",
    "resnest_contrib = 0.6\n",
    "_ = (deit_contrib*preds_DEIT+seresnext_contrib*preds_seresnext+resnest_contrib*preds_resnest50d)\n",
    "print(accuracy_score(labels.label,np.argmax(_,1)))\n",
    "confusion_matrix(labels.label,np.argmax(_,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res['b4_contrib'] = 0\n",
    "res['deit_contrib'] = 0\n",
    "res['seresnext_contrib'] = 0\n",
    "res ['Accuracy'] = 0\n",
    "res ['Adj_Accuracy'] = 0\n",
    "\n",
    "count = 0\n",
    "for i in np.arange(0,1,0.01):\n",
    "    for j in np.arange(0,1-i,0.01):\n",
    "        b4_contrib = i\n",
    "        deit_contrib = j\n",
    "        seresnext_contrib = 1-i-j\n",
    "        #print(i,j,1-i-j)\n",
    "        ens_preds_adj = (b4_contrib*preds_mean_effnetb4+seresnext_contrib*preds_mean_seresnext+deit_contrib*preds_mean_DEIT)\n",
    "        ens_preds = (b4_contrib*preds_effnet_b4+seresnext_contrib*preds_seresnext+deit_contrib*preds_DEIT)\n",
    "        res.loc[count,'b4_contrib'] = i\n",
    "        res.loc[count,'seresnext_contrib'] = 1-i-j\n",
    "        res.loc[count,'deit_contrib'] = j\n",
    "        res.loc[count,'Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds,1))\n",
    "        res.loc[count,'Adj_Accuracy'] = accuracy_score(labels.label,np.argmax(ens_preds_adj,1))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deit_contrib = 0.25\n",
    "seresnext_contrib = 0.46\n",
    "b4_contrib = 0.29\n",
    "_ = (b4_contrib*preds_effnet_b4+seresnext_contrib*preds_seresnext+deit_contrib*preds_DEIT)\n",
    "print(accuracy_score(labels.label,np.argmax(_,1)))\n",
    "confusion_matrix(labels.label,np.argmax(_,1),normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
