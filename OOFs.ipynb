{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import timm\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import albumentations\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from albumentations.core.composition import Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import cv2\n",
    "import torch\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "pytorch_lightning.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'C:\\Users\\Kaggle\\Leaf_Classification\\input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read csv and label mapping\n",
    "\n",
    "labels = pd.read_csv(os.path.join(base_dir,'train.csv'))\n",
    "\n",
    "with open(f'{base_dir}/label_num_to_disease_map.json') as f:\n",
    "    label_mapper = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_names: List,\n",
    "        transforms: Compose,\n",
    "        labels: Optional[List[int]],\n",
    "        img_path: str = '',\n",
    "        mode: str = 'train',\n",
    "        labels_to_ohe: bool = False,\n",
    "        n_classes: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Image classification dataset.\n",
    "\n",
    "        Args:\n",
    "            df: dataframe with image id and bboxes\n",
    "            mode: train/val/test\n",
    "            img_path: path to images\n",
    "            transforms: albumentations\n",
    "        \"\"\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        self.img_path = img_path\n",
    "        self.image_names = image_names\n",
    "        if labels is not None:\n",
    "            if not labels_to_ohe:\n",
    "                self.labels = np.array(labels)\n",
    "            else:\n",
    "                self.labels = np.zeros((len(labels), n_classes))\n",
    "                self.labels[np.arange(len(labels)), np.array(labels)] = 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, np.array]:\n",
    "        image_path = self.img_path + self.image_names[idx]\n",
    "        image = cv2.imread(f'{image_path}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(image_path)\n",
    "        target = self.labels[idx]\n",
    "\n",
    "        img = self.transforms(image=image)['image']\n",
    "        sample = {'image_path': image_path, 'image': img, 'target': np.array(target).astype('int64')}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 576\n",
    "sz_efffnetb4 = 600\n",
    "deit_size = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTA0_deit = albumentations.Compose([\n",
    "            albumentations.Resize(deit_size, deit_size,p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA1_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.HorizontalFlip(p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA2_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.VerticalFlip(p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA2_5_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.Transpose(p=1.),\n",
    "           albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA3_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=1\n",
    "            ),\n",
    "           albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA4_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=1.\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA5_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.Cutout(p=1.), \n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA6_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.ShiftScaleRotate(p=1.),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA7_deit = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(deit_size, deit_size),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=1.),\n",
    "            ToTensorV2()],\n",
    "            p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTA0 = albumentations.Compose([\n",
    "            albumentations.Resize(sz, sz,p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA1 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.HorizontalFlip(p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA2 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.VerticalFlip(p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA2_5 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Transpose(p=1.),\n",
    "           albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA3 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=1\n",
    "            ),\n",
    "           albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA4 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=1.\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "TTA5 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.Cutout(p=1.), \n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA6 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.ShiftScaleRotate(p=1.),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "TTA7 = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=1.),\n",
    "            ToTensorV2()],\n",
    "            p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_augs_effnet = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz_efffnetb4, sz_efffnetb4),\n",
    "            albumentations.Transpose(p=0.5),\n",
    "            albumentations.HorizontalFlip(p=0.5),\n",
    "            albumentations.VerticalFlip(p=0.5),\n",
    "            albumentations.ShiftScaleRotate(p=0.5),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=0.5),\n",
    "            albumentations.Cutout(p=0.5), \n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "\n",
    "\n",
    "infer_augs_resnext = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Transpose(p=0.5),\n",
    "            albumentations.HorizontalFlip(p=0.5),\n",
    "            albumentations.VerticalFlip(p=0.5),\n",
    "            albumentations.ShiftScaleRotate(p=0.5),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=0.5),\n",
    "            albumentations.Cutout(p=0.5), \n",
    "            ToTensorV2()],\n",
    "            p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_until(net: Any, param_name: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Freeze net until param_name\n",
    "\n",
    "\n",
    "    Args:\n",
    "        net:\n",
    "        param_name:\n",
    "\n",
    "    \"\"\"\n",
    "    found_name = False\n",
    "    for name, params in net.named_parameters():\n",
    "        if name == param_name:\n",
    "            found_name = True\n",
    "        params.requires_grad = found_name\n",
    "\n",
    "class BasicEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        arch: str = 'resnet18',\n",
    "        source: str = 'torchvision',\n",
    "        pretrained: str = None,\n",
    "        n_layers: int = -1,\n",
    "        freeze: bool = False,\n",
    "        to_one_channel: bool = False,\n",
    "        freeze_until_layer: str = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Encoder.\n",
    "\n",
    "        Args:\n",
    "            num_classes: the number of target classes, the size of the last layer's output\n",
    "            arch: the name of the architecture form pretrainedmodels\n",
    "            pretrained: the mode for pretrained model from pretrainedmodels\n",
    "            n_layers: number of layers to keep\n",
    "            freeze: to freeze model\n",
    "            freeze_until: freeze until this layer. If None, then freeze all layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if source == 'timm':\n",
    "            net = timm.create_model(arch, pretrained=pretrained)\n",
    "            self.output_dimension = list(net.children())[-1].in_features\n",
    "        else:\n",
    "            print(\"Not implemented\")\n",
    "            \n",
    "        if freeze:\n",
    "            freeze_until(net, freeze_until_layer)\n",
    "        \n",
    "        layers = list(net.children())[:n_layers]    \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDecoder(nn.Module):\n",
    "    def __init__(self, pool_output_size: int = 1, n_classes: int = 5, output_dimension: int = 512) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Decoder.\n",
    "\n",
    "        Args:\n",
    "            pool_output_size: the size of the result feature map after adaptive pooling layer\n",
    "            n_classes: n classes to output\n",
    "            output_dimension: output dimension of encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.pool = nn.AdaptiveAvgPool2d(output_size=pool_output_size)\n",
    "        self.fc = nn.Linear(output_dimension , n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.pool(x)\n",
    "        output = self.fc(x.view(x.size()[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDecoder_newhead(nn.Module):\n",
    "    def __init__(self, pool_output_size: int = 1, n_classes: int = 5, output_dimension: int = 512) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Decoder.\n",
    "\n",
    "        Args:\n",
    "            pool_output_size: the size of the result feature map after adaptive pooling layer\n",
    "            n_classes: n classes to output\n",
    "            output_dimension: output dimension of encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.pool = nn.AdaptiveAvgPool2d(output_size=pool_output_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(output_dimension, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.pool(x)\n",
    "        output = self.fc(x.view(x.size()[0], -1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_B4_newhead(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='tf_efficientnet_b4_ns',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_newhead(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_B3(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='tf_efficientnet_b3_ns',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_newhead(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_B4(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='tf_efficientnet_b4_ns',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_B5(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='tf_efficientnet_b5_ns',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDecoder_seresnext(nn.Module):\n",
    "    def __init__(self, pool_output_size: int = 1, n_classes: int = 5, output_dimension: int = 512) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Decoder.\n",
    "\n",
    "        Args:\n",
    "            pool_output_size: the size of the result feature map after adaptive pooling layer\n",
    "            n_classes: n classes to output\n",
    "            output_dimension: output dimension of encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.pool = nn.AdaptiveAvgPool2d(output_size=pool_output_size)\n",
    "        #self.fc = nn.Linear(output_dimension , n_classes)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(output_dimension, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 5),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #x = self.pool(x)\n",
    "        output = self.fc(x.view(x.size()[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seResnextNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='seresnext50_32x4d',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_seresnext(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnextNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='resnext50_32x4d',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnextNet50d(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='resnext50d_32x4d',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_newhead(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnest50d(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='resnest50d_1s4x24d',\n",
    "                                    source='timm',\n",
    "                                    pretrained=False,\n",
    "                                    n_layers=-1,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder_newhead(1,5,self.encoder.output_dimension)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        logits = self.decoder(out)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEIT(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_384', pretrained=True)\n",
    "        output_dimension = list(self.encoder.children())[-1].in_features\n",
    "\n",
    "        self.encoder.head = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 5),\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    def forward(self, x, targets):\n",
    "        logits = self.encoder(x)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCassava(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super(LitCassava, self).__init__()\n",
    "        self.model = model\n",
    "        self.metric = pl.metrics.Accuracy()\n",
    "        self.learning_rate = 1e-4\n",
    "\n",
    "    def forward(self, x, targets, *args, **kwargs):\n",
    "        return self.model(x, targets)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)\n",
    "\n",
    "        return (\n",
    "            [optimizer],\n",
    "            [{'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'valid_loss'}],\n",
    "        )\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: torch.Tensor, batch_idx: int\n",
    "    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n",
    "        image = batch['image']\n",
    "        target = batch['target']\n",
    "        logits, loss = self(image, target)\n",
    "        score = self.metric(logits.argmax(1), target)\n",
    "        logs = {'train_loss': loss, f'train_accuracy': score}\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'log': logs,\n",
    "            'progress_bar': logs,\n",
    "            'logits': logits,\n",
    "            'target': target,\n",
    "            f'train_accuracy': score,\n",
    "        }\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        y_true = torch.cat([x['target'] for x in outputs])\n",
    "        y_pred = torch.cat([x['logits'] for x in outputs])\n",
    "        score = self.metric(y_pred.argmax(1), y_true)\n",
    "        \n",
    "        logs = {'train_loss': avg_loss, 'train_accuracy': score}\n",
    "        return {'log': logs, 'progress_bar': logs}\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: torch.Tensor, batch_idx: int\n",
    "    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n",
    "        image = batch['image']\n",
    "        target = batch['target']\n",
    "        logits, loss = self(image, target)\n",
    "        score = self.metric(logits.argmax(1), target)\n",
    "        logs = {'valid_loss': loss, f'valid_accuracy': score}\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'log': logs,\n",
    "            'progress_bar': logs,\n",
    "            'logits': logits,\n",
    "            'target': target,\n",
    "            f'valid_accuracy': score,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        y_true = torch.cat([x['target'] for x in outputs])\n",
    "        y_pred = torch.cat([x['logits'] for x in outputs])\n",
    "        score = self.metric(y_pred.argmax(1), y_true)\n",
    "\n",
    "        # score = torch.tensor(1.0, device=self.device)\n",
    "        logs = {'valid_loss': avg_loss, f'valid_accuracy': score, 'accuracy': score}\n",
    "        return {'valid_loss': avg_loss, 'log': logs, 'progress_bar': logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"../input/cassava-leaf-disease-classification/\"\n",
    "# sub = pd.read_csv(f'{path}/sample_submission.csv')\n",
    "# sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getloader(df,path,bs=8,fold=0,tta=0,deit=False):\n",
    "    \n",
    "    if not deit:\n",
    "        if tta == 0:\n",
    "            augs = TTA0\n",
    "        if tta == 1:\n",
    "            augs = TTA1\n",
    "        if tta == 2:\n",
    "            augs = TTA2\n",
    "        if tta == 3:\n",
    "            augs = TTA2_5\n",
    "        if tta == 4:\n",
    "            augs = TTA3\n",
    "        if tta == 5:\n",
    "            augs = TTA4\n",
    "        if tta == 6:\n",
    "            augs = TTA5\n",
    "        if tta == 7:\n",
    "            augs = TTA6\n",
    "        if tta == 8:\n",
    "            augs = TTA7\n",
    "    else:\n",
    "        if tta == 0:\n",
    "            augs = TTA0_deit\n",
    "        if tta == 1:\n",
    "            augs = TTA1_deit\n",
    "        if tta == 2:\n",
    "            augs = TTA2_deit\n",
    "        if tta == 3:\n",
    "            augs = TTA2_5_deit\n",
    "        if tta == 4:\n",
    "            augs = TTA3_deit\n",
    "        if tta == 5:\n",
    "            augs = TTA4_deit\n",
    "        if tta == 6:\n",
    "            augs = TTA5_deit\n",
    "        if tta == 7:\n",
    "            augs = TTA6_deit\n",
    "        if tta == 8:\n",
    "            augs = TTA7_deit\n",
    "    \n",
    "    print(f\"Using TTA: {tta} and DEIT:{deit}\")\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    train_indexes, valid_indexes = list(folds.split(df, df['label']))[fold]\n",
    "\n",
    "    train_df = df.iloc[train_indexes]\n",
    "    valid_df = df.iloc[valid_indexes]\n",
    "\n",
    "\n",
    "    train_dataset = ImageClassificationDataset(image_names=train_df['image_id'].values,\n",
    "                                                    transforms=augs,\n",
    "                                                    labels=train_df['label'].values,\n",
    "                                                    img_path=path,\n",
    "                                                    mode='train',\n",
    "                                                    labels_to_ohe=False,\n",
    "                                                    n_classes=5)\n",
    "    valid_dataset = ImageClassificationDataset(image_names=valid_df['image_id'].values,\n",
    "                                                    transforms=augs,\n",
    "                                                    labels=valid_df['label'].values,\n",
    "                                                    img_path=path,\n",
    "                                                    mode='valid',\n",
    "                                                    labels_to_ohe=False,\n",
    "                                                    n_classes=5)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=bs,num_workers=0,shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=bs,num_workers=0,shuffle=False)\n",
    "\n",
    "    return train_loader,valid_loader,valid_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = os.listdir('../input/leaves-tf-enet-b4-ns-576-finetune-temperedloss/')\n",
    "# modelsPath = [os.path.join('../input/leaves-tf-enet-b4-ns-576-finetune-temperedloss',i) for i in models]\n",
    "# modelsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def getPreds(test_loader,lit_model,logitsOut=True):\n",
    "    predictions = []\n",
    "    \n",
    "    for batch in tqdm(test_loader):\n",
    "        image = batch['image'].to('cuda')\n",
    "        target = batch['target'].to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = lit_model.model(image, target)[0]\n",
    "            _outputs = outputs.softmax(1).detach().cpu().numpy()\n",
    "            preds = outputs.argmax(1).detach().cpu().numpy()\n",
    "            if not logitsOut:\n",
    "                predictions.append(preds)\n",
    "            else:\n",
    "                #predictions[range(len(_outputs))] += _outputs\n",
    "                predictions.append(_outputs)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for DEIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\DEIT_Mixup_without_hesitation_After_Epoch10\\avgWeights\\_Avg_fold_4.ckpt'}\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_DEIT = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = DEIT()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta,deit=True)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_DEIT[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_DEIT = preds_DEIT.copy()\n",
    "preds_mean_DEIT[:,0]= preds_DEIT[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_DEIT[:,1]= preds_DEIT[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_DEIT[:,2]= preds_DEIT[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_DEIT[:,3]= preds_DEIT[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_DEIT[:,4]= preds_DEIT[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_DEIT,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_DEIT,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for effnet B3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelsDir = os.listdir(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\'')\n",
    "\n",
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b3_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'}\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelsDir = os.listdir(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\)\n",
    "\n",
    "# modelsPath = [os.path.join(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\best_ones',i) for i in models]\n",
    "# modelsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_effnet_b3 = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B3()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b3[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_effnetb3 = preds_effnet_b3.copy()\n",
    "preds_mean_effnetb3[:,0]= preds_effnet_b3[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_effnetb3[:,1]= preds_effnet_b3[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_effnetb3[:,2]= preds_effnet_b3[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_effnetb3[:,3]= preds_effnet_b3[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_effnetb3[:,4]= preds_effnet_b3[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_effnetb3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_effnet_b3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for seresnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelsDir = os.listdir(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\'')\n",
    "\n",
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\seresnext50_32x4d_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_seresnext = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):\n",
    "    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = seResnextNet()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=128,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_seresnext[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_seresnext = preds_seresnext.copy()\n",
    "preds_mean_seresnext[:,0]= preds_mean_seresnext[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_seresnext[:,1]= preds_mean_seresnext[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_seresnext[:,2]= preds_mean_seresnext[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_seresnext[:,3]= preds_mean_seresnext[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_seresnext[:,4]= preds_mean_seresnext[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_seresnext,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classOut = np.argmax(preds_seresnext,1)\n",
    "# classOut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_seresnext,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for Effnet b4 Taylor Loss - 2 SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\Top 2\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_effnet_b4_taylor = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B4_newhead()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b4_taylor[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_effnetb4_taylor = preds_effnet_b4_taylor.copy()\n",
    "preds_mean_effnetb4_taylor[:,0]= preds_effnet_b4_taylor[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_effnetb4_taylor[:,1]= preds_effnet_b4_taylor[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_effnetb4_taylor[:,2]= preds_effnet_b4_taylor[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_effnetb4_taylor[:,3]= preds_effnet_b4_taylor[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_effnetb4_taylor[:,4]= preds_effnet_b4_taylor[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_effnetb4_taylor,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_effnet_b4_taylor,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect();torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for Effnet b4 Taylor Loss - 3 SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_ns_sz_576_Mixup_0.3_TaylorLoss_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_effnet_b4_taylor_3SWA = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B4_newhead()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b4_taylor_3SWA[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_effnetb4_taylor_3SWA = preds_effnet_b4_taylor_3SWA.copy()\n",
    "preds_mean_effnetb4_taylor_3SWA[:,0]= preds_effnet_b4_taylor_3SWA[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_effnetb4_taylor_3SWA[:,1]= preds_effnet_b4_taylor_3SWA[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_effnetb4_taylor_3SWA[:,2]= preds_effnet_b4_taylor_3SWA[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_effnetb4_taylor_3SWA[:,3]= preds_effnet_b4_taylor_3SWA[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_effnetb4_taylor_3SWA[:,4]= preds_effnet_b4_taylor_3SWA[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_effnetb4_taylor_3SWA,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_effnet_b4_taylor_3SWA,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect();torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for B4 focal cosine loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def getPreds(test_loader,lit_model,logitsOut=True):\n",
    "    predictions = []\n",
    "    \n",
    "    for batch in tqdm(test_loader):\n",
    "        image = batch['image'].to('cuda')\n",
    "        target = batch['target'].to('cuda')\n",
    "        with torch.no_grad():\n",
    "            outputs = lit_model.model(image, target)[0]\n",
    "            _outputs = outputs.detach().cpu().numpy()\n",
    "            preds = outputs.argmax(1).detach().cpu().numpy()\n",
    "            if not logitsOut:\n",
    "                predictions.append(preds)\n",
    "            else:\n",
    "                #predictions[range(len(_outputs))] += _outputs\n",
    "                predictions.append(_outputs)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b4_sz_576_Mixup_0.3_focalCosinesLoss_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for fold:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 0 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:34<00:00,  1.41it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 1 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:35<00:00,  1.40it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 2 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 3 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:35<00:00,  1.40it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 4 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:42<00:00,  1.31it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for fold:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 0 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:35<00:00,  1.40it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 1 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:34<00:00,  1.41it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 2 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:34<00:00,  1.41it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 3 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 4 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:41<00:00,  1.32it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for fold:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 0 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 1 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 2 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 3 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 4 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:42<00:00,  1.31it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for fold:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 0 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 1 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.40it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 2 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 3 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 4 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:42<00:00,  1.31it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for fold:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 0 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 1 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 2 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 3 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:36<00:00,  1.39it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TTA: 4 and DEIT:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 134/134 [01:42<00:00,  1.31it/s]\n",
      "C:\\Users\\Anaconda_Stuff\\envs\\Pytorch_17_cuda11\\lib\\site-packages\\ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "preds_effnet_b4 = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):    \n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B4_newhead()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)      \n",
    "#    lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b4[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_effnetb4 = preds_effnet_b4.copy()\n",
    "preds_mean_effnetb4[:,0]= preds_effnet_b4[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_effnetb4[:,1]= preds_effnet_b4[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_effnetb4[:,2]= preds_effnet_b4[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_effnetb4[:,3]= preds_effnet_b4[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_effnetb4[:,4]= preds_effnet_b4[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_effnetb4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.901060896387344, 0.9008739542926578)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_effnet_b4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect();torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preds for B5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_effnet_b5 = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):\n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = Net_B5()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_effnet_b5[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_effnetb5 = preds_effnet_b5.copy()\n",
    "preds_mean_effnetb5[:,0]= preds_effnet_b5[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_effnetb5[:,1]= preds_effnet_b5[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_effnetb5[:,2]= preds_effnet_b5[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_effnetb5[:,3]= preds_effnet_b5[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_effnetb5[:,4]= preds_effnet_b5[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_effnetb5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_effnet_b5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del _,softmaxOut,model,lit_model,checkpoint,test_loader; gc.collect();torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resnest50d_1s4x24d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnest50d_1s4x24d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_resnest50d = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):\n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = resnest50d()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "    lit_model.load_state_dict(checkpoint)  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_resnest50d[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_resnest50d = preds_resnest50d.copy()\n",
    "preds_mean_resnest50d[:,0]= preds_resnest50d[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_resnest50d[:,1]= preds_resnest50d[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_resnest50d[:,2]= preds_resnest50d[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_resnest50d[:,3]= preds_resnest50d[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_resnest50d[:,4]= preds_resnest50d[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_resnest50d,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_resnest50d,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnext50D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "              1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "              2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "              3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "              4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\resnext50_32x4d_sz_576_Mixup_0.3_timm_newhead\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "             }\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_resnext50d = np.zeros((len(labels),5))\n",
    "\n",
    "train_img_path = 'C:/Users/Kaggle/Leaf_Classification/input/train_images/'\n",
    "TTA = 5\n",
    "for f in range(5):\n",
    "    softmaxOut = 0.\n",
    "    print(f\"Prediction for fold:{f}\")\n",
    "    modelPath = modelsPath[f]\n",
    "    model = ResnextNet50d()\n",
    "    lit_model = LitCassava(model)\n",
    "    checkpoint = torch.load(modelPath, map_location=lambda storage, loc: storage)\n",
    "#     lit_model.load_state_dict(checkpoint['state_dict'])  \n",
    "    lit_model.load_state_dict(checkpoint)  \n",
    "    lit_model.model.eval()\n",
    "    lit_model.model.cuda()\n",
    "    for tta in range(TTA):\n",
    "        _,test_loader,valid_ix = getloader(labels,path=train_img_path,bs=32,fold=f,tta=tta)\n",
    "        predictions = []\n",
    "        softmaxOut+= (getPreds(test_loader,lit_model,logitsOut=True))\n",
    "    preds_resnext50d[valid_ix,:] = np.vstack(softmaxOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean_resnext50d = preds_resnext50d.copy()\n",
    "preds_mean_resnext50d[:,0]= preds_resnext50d[:,0]/(1+np.sqrt(0.05))\n",
    "preds_mean_resnext50d[:,1]= preds_resnext50d[:,1]/(1+np.sqrt(0.102))\n",
    "preds_mean_resnext50d[:,2]= preds_resnext50d[:,2]/(1+np.sqrt(0.111))\n",
    "preds_mean_resnext50d[:,3]= preds_resnext50d[:,3]/(1+np.sqrt(0.614)) #### not the exact frequency stats here, instead of 0.614, we have used 0.4 \n",
    "preds_mean_resnext50d[:,4]= preds_resnext50d[:,4]/(1+np.sqrt(0.12))\n",
    "\n",
    "classOut = np.argmax(preds_mean_resnext50d,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels.label,classOut),accuracy_score(labels.label,np.argmax(preds_resnest50d,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.52417183,  3.63484025,  2.63429618,  0.07504719,  5.96637344])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_effnet_b4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.304834</td>\n",
       "      <td>0.726968</td>\n",
       "      <td>0.526859</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>1.193275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.462400</td>\n",
       "      <td>-0.331430</td>\n",
       "      <td>-0.459014</td>\n",
       "      <td>64.639600</td>\n",
       "      <td>0.212227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.209083</td>\n",
       "      <td>0.157383</td>\n",
       "      <td>0.252195</td>\n",
       "      <td>0.921993</td>\n",
       "      <td>10.032390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.105837</td>\n",
       "      <td>9.464127</td>\n",
       "      <td>-0.129015</td>\n",
       "      <td>0.265302</td>\n",
       "      <td>0.027899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.790286</td>\n",
       "      <td>-0.533531</td>\n",
       "      <td>-0.683876</td>\n",
       "      <td>99.227258</td>\n",
       "      <td>0.192690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label  fold         0         1         2          3  \\\n",
       "0  1000015157.jpg      0     3  2.304834  0.726968  0.526859   0.015009   \n",
       "1  1000201771.jpg      3     2 -0.462400 -0.331430 -0.459014  64.639600   \n",
       "2   100042118.jpg      1     2 -0.209083  0.157383  0.252195   0.921993   \n",
       "3  1000723321.jpg      1     1 -0.105837  9.464127 -0.129015   0.265302   \n",
       "4  1000812911.jpg      3     2 -0.790286 -0.533531 -0.683876  99.227258   \n",
       "\n",
       "           4  \n",
       "0   1.193275  \n",
       "1   0.212227  \n",
       "2  10.032390  \n",
       "3   0.027899  \n",
       "4   0.192690  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OOF to pd df\n",
    "from scipy.special import softmax\n",
    "# OOF_effnetb4 = pd.DataFrame(softmax(preds_effnet_b4/5,1))\n",
    "OOF_effnetb4 = pd.DataFrame(preds_effnet_b4/5)\n",
    "OOF_effnetb4['image_id'] = labels['image_id']\n",
    "OOF_effnetb4['label'] = labels['label']\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "OOF_effnetb4['fold'] = 0\n",
    "OOF_effnetb4.loc[list(folds.split(labels, labels['label']))[0][1],'fold'] = 0\n",
    "OOF_effnetb4.loc[list(folds.split(labels, labels['label']))[1][1],'fold'] = 1\n",
    "OOF_effnetb4.loc[list(folds.split(labels, labels['label']))[2][1],'fold'] = 2\n",
    "OOF_effnetb4.loc[list(folds.split(labels, labels['label']))[3][1],'fold'] = 3\n",
    "OOF_effnetb4.loc[list(folds.split(labels, labels['label']))[4][1],'fold'] = 4\n",
    "OOF_effnetb4= OOF_effnetb4[['image_id','label','fold',0,1,2,3,4]]\n",
    "OOF_effnetb4['fold'].value_counts()\n",
    "OOF_effnetb4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OOF to pd df\n",
    "OOF_DEIT = pd.DataFrame(preds_DEIT/5)\n",
    "OOF_DEIT['image_id'] = labels['image_id']\n",
    "OOF_DEIT['label'] = labels['label']\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "OOF_DEIT['fold'] = 0\n",
    "OOF_DEIT.loc[list(folds.split(labels, labels['label']))[0][1],'fold'] = 0\n",
    "OOF_DEIT.loc[list(folds.split(labels, labels['label']))[1][1],'fold'] = 1\n",
    "OOF_DEIT.loc[list(folds.split(labels, labels['label']))[2][1],'fold'] = 2\n",
    "OOF_DEIT.loc[list(folds.split(labels, labels['label']))[3][1],'fold'] = 3\n",
    "OOF_DEIT.loc[list(folds.split(labels, labels['label']))[4][1],'fold'] = 4\n",
    "OOF_DEIT= OOF_DEIT[['image_id','label','fold',0,1,2,3,4]]\n",
    "OOF_DEIT['fold'].value_counts()\n",
    "OOF_DEIT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OOF to pd df\n",
    "OOF_seresnext = pd.DataFrame(preds_seresnext/5)\n",
    "OOF_seresnext['image_id'] = labels['image_id']\n",
    "OOF_seresnext['label'] = labels['label']\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "OOF_seresnext['fold'] = 0\n",
    "OOF_seresnext.loc[list(folds.split(labels, labels['label']))[0][1],'fold'] = 0\n",
    "OOF_seresnext.loc[list(folds.split(labels, labels['label']))[1][1],'fold'] = 1\n",
    "OOF_seresnext.loc[list(folds.split(labels, labels['label']))[2][1],'fold'] = 2\n",
    "OOF_seresnext.loc[list(folds.split(labels, labels['label']))[3][1],'fold'] = 3\n",
    "OOF_seresnext.loc[list(folds.split(labels, labels['label']))[4][1],'fold'] = 4\n",
    "OOF_seresnext= OOF_seresnext[['image_id','label','fold',0,1,2,3,4]]\n",
    "OOF_seresnext['fold'].value_counts()\n",
    "OOF_seresnext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOF_seresnext.to_csv(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\OOFs\\OOF_seresnext_5TTA_576.csv',index=False)\n",
    "OOF_DEIT.to_csv(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\OOFs\\OOF_DEIT_5TTA_384.csv',index=False)\n",
    "OOF_effnetb4.to_csv(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\OOFs\\OOF_effnetb4_5TTA_576.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOF_effnetb4.to_csv(r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\OOFs\\OOF_effnetb4_5TTA_576_logits.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
