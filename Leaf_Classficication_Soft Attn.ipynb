{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import albumentations\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from albumentations.core.composition import Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from typing import Any, Dict, List, Union, Optional\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import cv2\n",
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n",
    "import timm\n",
    "from PIL import Image\n",
    "from PIL import ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning\n",
    "pytorch_lightning.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pytorch_lightning.utilities.seed.seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'C:\\Users\\Kaggle\\Leaf_Classification\\input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read csv and label mapping\n",
    "\n",
    "labels = pd.read_csv(os.path.join(base_dir,'train.csv'))\n",
    "\n",
    "with open(f'{base_dir}/label_num_to_disease_map.json') as f:\n",
    "    label_mapper = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_names: List,\n",
    "        transforms: Compose,\n",
    "        labels: Optional[List[int]],\n",
    "        img_path: str = '',\n",
    "        mode: str = 'train',\n",
    "        labels_to_ohe: bool = False,\n",
    "        n_classes: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Image classification dataset.\n",
    "\n",
    "        Args:\n",
    "            df: dataframe with image id and bboxes\n",
    "            mode: train/val/test\n",
    "            img_path: path to images\n",
    "            transforms: albumentations\n",
    "        \"\"\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        self.img_path = img_path\n",
    "        self.image_names = image_names\n",
    "        if labels is not None:\n",
    "            if not labels_to_ohe:\n",
    "                self.labels = np.array(labels)\n",
    "            else:\n",
    "                self.labels = np.zeros((len(labels), n_classes))\n",
    "                self.labels[np.arange(len(labels)), np.array(labels)] = 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, np.array]:\n",
    "        image_path = self.img_path + self.image_names[idx]\n",
    "        image = cv2.imread(f'{image_path}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(image_path)\n",
    "        target = self.labels[idx]\n",
    "\n",
    "        img = self.transforms(image=image)['image']\n",
    "        sample = {'image_path': image_path, 'image': img, 'target': np.array(target).astype('int64')}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 576\n",
    "sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentations taken from: https://www.kaggle.com/khyeh0719/pytorch-efficientnet-baseline-train-amp-aug\n",
    "train_augs = albumentations.Compose([\n",
    "            albumentations.RandomResizedCrop(sz, sz),\n",
    "            albumentations.Transpose(p=0.5),\n",
    "            albumentations.HorizontalFlip(p=0.5),\n",
    "            albumentations.VerticalFlip(p=0.5),\n",
    "            albumentations.ShiftScaleRotate(p=0.5),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "            albumentations.CoarseDropout(p=0.5),\n",
    "            albumentations.Cutout(p=0.5), \n",
    "            ToTensorV2()],\n",
    "            p=1.)\n",
    "  \n",
    "        \n",
    "valid_augs = albumentations.Compose([\n",
    "            albumentations.CenterCrop(sz, sz, p=1.),\n",
    "            albumentations.Resize(sz, sz),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0)\n",
    "            ,ToTensorV2()], p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CutMix, Mixup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL Data Module - https://www.kaggle.com/artgor/cassava-disease-identification-with-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 train_augs,\n",
    "                 valid_augs,\n",
    "                 path,\n",
    "                bs=8,\n",
    "                fold=0):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.train_augs = train_augs\n",
    "        self.valid_augs = valid_augs\n",
    "        self.path = path\n",
    "        self.bs = bs\n",
    "        self.fold = fold\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        train_indexes, valid_indexes = list(folds.split(self.df, self.df['label']))[self.fold]\n",
    "        \n",
    "        train_df = self.df.iloc[train_indexes]\n",
    "        valid_df = self.df.iloc[valid_indexes]\n",
    "\n",
    "        \n",
    "        self.train_dataset = ImageClassificationDataset(image_names=train_df['image_id'].values,\n",
    "                                                        transforms=train_augs,\n",
    "                                                        labels=train_df['label'].values,\n",
    "                                                        img_path=self.path,\n",
    "                                                        mode='train',\n",
    "                                                        labels_to_ohe=False,\n",
    "                                                        n_classes=5)\n",
    "        self.valid_dataset = ImageClassificationDataset(image_names=valid_df['image_id'].values,\n",
    "                                                        transforms=valid_augs,\n",
    "                                                        labels=valid_df['label'].values,\n",
    "                                                        img_path=self.path,\n",
    "                                                        mode='valid',\n",
    "                                                        labels_to_ohe=False,\n",
    "                                                        n_classes=5)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.bs,\n",
    "            num_workers=0,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.bs,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        return valid_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal Cosine Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLoss(nn.Module):\n",
    "    def __init__(self, xent=.1, reduction=\"mean\"):\n",
    "        super(CosineLoss, self).__init__()\n",
    "        self.xent = xent\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        self.y = torch.Tensor([1]).cuda()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        cosine_loss = F.cosine_embedding_loss(input, F.one_hot(target, num_classes=input.size(-1)), self.y, reduction=self.reduction)\n",
    "        cent_loss = F.cross_entropy(F.normalize(input), target, reduction=self.reduction)\n",
    "        \n",
    "        return cosine_loss + self.xent * cent_loss\n",
    "\n",
    "class FocalCosineLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, xent=.1, reduction=\"mean\"):\n",
    "        super(FocalCosineLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.xent = xent\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        self.y = torch.Tensor([1]).cuda()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        cosine_loss = F.cosine_embedding_loss(input, F.one_hot(target, num_classes=input.size(-1)), self.y, reduction=self.reduction)\n",
    "\n",
    "        cent_loss = F.cross_entropy(F.normalize(input), target, reduce=False)\n",
    "        pt = torch.exp(-cent_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * cent_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            focal_loss = torch.mean(focal_loss)\n",
    "        \n",
    "        return cosine_loss + self.xent * focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define encoder - https://www.kaggle.com/artgor/cassava-disease-identification-with-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_until(net: Any, param_name: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Freeze net until param_name\n",
    "\n",
    "\n",
    "    Args:\n",
    "        net:\n",
    "        param_name:\n",
    "\n",
    "    \"\"\"\n",
    "    found_name = False\n",
    "    for name, params in net.named_parameters():\n",
    "        if name == param_name:\n",
    "            found_name = True\n",
    "        params.requires_grad = found_name\n",
    "\n",
    "class BasicEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        arch: str = 'resnet18',\n",
    "        source: str = 'torchvision',\n",
    "        pretrained: str = None,\n",
    "        n_layers: int = -1,\n",
    "        freeze: bool = False,\n",
    "        to_one_channel: bool = False,\n",
    "        freeze_until_layer: str = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Encoder.\n",
    "\n",
    "        Args:\n",
    "            num_classes: the number of target classes, the size of the last layer's output\n",
    "            arch: the name of the architecture form pretrainedmodels\n",
    "            pretrained: the mode for pretrained model from pretrainedmodels\n",
    "            n_layers: number of layers to keep\n",
    "            freeze: to freeze model\n",
    "            freeze_until: freeze until this layer. If None, then freeze all layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if source == 'timm':\n",
    "            net = timm.create_model(arch, pretrained=pretrained)\n",
    "            self.output_dimension = list(net.children())[-1].in_features\n",
    "        else:\n",
    "            print(\"Not implemented\")\n",
    "            \n",
    "        if freeze:\n",
    "            freeze_until(net, freeze_until_layer)\n",
    "        \n",
    "        layers = list(net.children())[:n_layers]    \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDecoder(nn.Module):\n",
    "    def __init__(self, pool_output_size: int = 1, n_classes: int = 5, output_dimension: int = 512) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Decoder.\n",
    "\n",
    "        Args:\n",
    "            pool_output_size: the size of the result feature map after adaptive pooling layer\n",
    "            n_classes: n classes to output\n",
    "            output_dimension: output dimension of encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.pool = nn.AdaptiveAvgPool2d(output_size=pool_output_size)\n",
    "#         self.fc = nn.Linear(output_dimension , n_classes)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(output_dimension, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 5),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #x = self.pool(x)\n",
    "        output = self.fc(x.view(x.size()[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://www.kaggle.com/jy2tong/efficientnet-b2-soft-attention\n",
    "\n",
    "class PAM_Module(nn.Module):\n",
    "    \"\"\" Position attention module\"\"\"\n",
    "    #Ref from SAGAN\n",
    "    def __init__(self, in_dim):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X (HxW) X (HxW)\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class CAM_Module(nn.Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
    "        attention = torch.softmax(energy_new, dim=-1)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        # def __init__(self):\n",
    "        super(CBAM, self).__init__()\n",
    "        inter_channels = in_channels // 4\n",
    "        self.conv1_c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(inter_channels),\n",
    "                                     nn.ReLU())\n",
    "        \n",
    "        self.conv1_s = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(inter_channels),\n",
    "                                     nn.ReLU())\n",
    "\n",
    "        self.channel_gate = CAM_Module(inter_channels)\n",
    "        self.spatial_gate = PAM_Module(inter_channels)\n",
    "\n",
    "        self.conv2_c = nn.Sequential(nn.Conv2d(inter_channels, in_channels, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(in_channels),\n",
    "                                     nn.ReLU())\n",
    "        self.conv2_a = nn.Sequential(nn.Conv2d(inter_channels, in_channels, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(in_channels),\n",
    "                                     nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat1 = self.conv1_c(x)\n",
    "        chnl_att = self.channel_gate(feat1)\n",
    "        chnl_att = self.conv2_c(chnl_att)\n",
    "\n",
    "        feat2 = self.conv1_s(x)\n",
    "        spat_att = self.spatial_gate(feat2)\n",
    "        spat_att = self.conv2_a(spat_att)\n",
    "\n",
    "        x_out = chnl_att + spat_att\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def adaptive_pool_feat_mult(pool_type='avg'):\n",
    "    if pool_type == 'catavgmax':\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def adaptive_avgmax_pool2d(x, output_size=1):\n",
    "    x_avg = F.adaptive_avg_pool2d(x, output_size)\n",
    "    x_max = F.adaptive_max_pool2d(x, output_size)\n",
    "    return 0.5 * (x_avg + x_max)\n",
    "\n",
    "\n",
    "def adaptive_catavgmax_pool2d(x, output_size=1):\n",
    "    x_avg = F.adaptive_avg_pool2d(x, output_size)\n",
    "    x_max = F.adaptive_max_pool2d(x, output_size)\n",
    "    return torch.cat((x_avg, x_max), 1)\n",
    "\n",
    "\n",
    "def select_adaptive_pool2d(x, pool_type='avg', output_size=1):\n",
    "    \"\"\"Selectable global pooling function with dynamic input kernel size\n",
    "    \"\"\"\n",
    "    if pool_type == 'avg':\n",
    "        x = F.adaptive_avg_pool2d(x, output_size)\n",
    "    elif pool_type == 'avgmax':\n",
    "        x = adaptive_avgmax_pool2d(x, output_size)\n",
    "    elif pool_type == 'catavgmax':\n",
    "        x = adaptive_catavgmax_pool2d(x, output_size)\n",
    "    elif pool_type == 'max':\n",
    "        x = F.adaptive_max_pool2d(x, output_size)\n",
    "    else:\n",
    "        assert False, 'Invalid pool type: %s' % pool_type\n",
    "    return x\n",
    "\n",
    "\n",
    "class AdaptiveAvgMaxPool2d(nn.Module):\n",
    "    def __init__(self, output_size=1):\n",
    "        super(AdaptiveAvgMaxPool2d, self).__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return adaptive_avgmax_pool2d(x, self.output_size)\n",
    "\n",
    "\n",
    "class AdaptiveCatAvgMaxPool2d(nn.Module):\n",
    "    def __init__(self, output_size=1):\n",
    "        super(AdaptiveCatAvgMaxPool2d, self).__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return adaptive_catavgmax_pool2d(x, self.output_size)\n",
    "\n",
    "\n",
    "class SelectAdaptivePool2d(nn.Module):\n",
    "    \"\"\"Selectable global pooling layer with dynamic input kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size=1, pool_type='avg', flatten=False):\n",
    "        super(SelectAdaptivePool2d, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.pool_type = pool_type\n",
    "        self.flatten = flatten\n",
    "        if pool_type == 'avgmax':\n",
    "            self.pool = AdaptiveAvgMaxPool2d(output_size)\n",
    "        elif pool_type == 'catavgmax':\n",
    "            self.pool = AdaptiveCatAvgMaxPool2d(output_size)\n",
    "        elif pool_type == 'max':\n",
    "            self.pool = nn.AdaptiveMaxPool2d(output_size)\n",
    "        else:\n",
    "            if pool_type != 'avg':\n",
    "                assert False, 'Invalid pool type: %s' % pool_type\n",
    "            self.pool = nn.AdaptiveAvgPool2d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(1)\n",
    "        return x\n",
    "\n",
    "    def feat_mult(self):\n",
    "        return adaptive_pool_feat_mult(self.pool_type)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + 'output_size=' + str(self.output_size) \\\n",
    "               + ', pool_type=' + self.pool_type + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Model class.\n",
    "\n",
    "        Args:\n",
    "            cfg: main config\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = BasicEncoder(arch='tf_efficientnet_b4_ns',\n",
    "                                    source='timm',\n",
    "                                    pretrained=True,\n",
    "                                    n_layers=-2,\n",
    "                                    freeze=False,\n",
    "                                    to_one_channel=False,\n",
    "                                    freeze_until_layer=None)\n",
    "        self.decoder = BasicDecoder(1,5,self.encoder.output_dimension*2)\n",
    "        self.local_fe = CBAM(self.encoder.output_dimension)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.pool = SelectAdaptivePool2d()\n",
    "        #self.loss = nn.CrossEntropyLoss()\n",
    "        self.loss = FocalCosineLoss()\n",
    "        \n",
    "    def forward(self, x, targets):\n",
    "        out = self.encoder(x)\n",
    "        \n",
    "        # use default's global features\n",
    "        global_feas = self.pool(out)\n",
    "        global_feas = global_feas.flatten(start_dim=1)\n",
    "        global_feas = self.dropout(global_feas)\n",
    "        \n",
    "        local_feas = self.local_fe(out)\n",
    "        local_feas = torch.sum(local_feas, dim=[2,3])\n",
    "        local_feas = self.dropout(local_feas)\n",
    "        \n",
    "        all_feas = torch.cat([global_feas, local_feas], dim=1)\n",
    "#         outputs = self.classifier(all_feas)\n",
    "        \n",
    "        logits = self.decoder(all_feas)\n",
    "        loss = self.loss(logits, targets).view(1)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCassava(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super(LitCassava, self).__init__()\n",
    "        self.model = model\n",
    "        self.metric = pl.metrics.Accuracy()\n",
    "        self.learning_rate = 1e-4 #1e-4\n",
    "        \n",
    "    def forward(self, x, targets, *args, **kwargs):\n",
    "        return self.model(x, targets)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate, weight_decay=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)\n",
    "#         scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr=0.05,epochs = 25,steps_per_epoch=1784,verbose=False)\n",
    "        #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4, eta_min=1e-6, last_epoch=-1, verbose=True)\n",
    "        #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=4, T_mult=2, eta_min=1e-6, last_epoch=-1, verbose=True)\n",
    "        return (\n",
    "            [optimizer],\n",
    "            [{'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'valid_loss'}],\n",
    "#             [{'scheduler': scheduler, 'interval': 'step', 'monitor': 'valid_loss'}],\n",
    "        )\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: torch.Tensor, batch_idx: int\n",
    "    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n",
    "        image = batch['image']\n",
    "        target = batch['target']\n",
    "        \n",
    "        ### add in cutmix+mixup+cutout\n",
    "        const = np.random.randint(10)\n",
    "        if const<3:\n",
    "            # mixup\n",
    "            #print(\"Mixup envoked\")\n",
    "            mixed_input, targets_a, targets_b, lam = mixup_data(image, target,alpha=0.4)\n",
    "            logits,loss_a = self(mixed_input,targets_a)\n",
    "            logits,loss_b = self(mixed_input,targets_b)\n",
    "            loss = lam * loss_a + (1 - lam) * loss_b\n",
    "            #rint(\"Mixup loss\")\n",
    "        else:\n",
    "            logits, loss = self(image, target)\n",
    "            #rint(\"Mixup not envoked\")\n",
    "        \n",
    "        print(loss,loss.dtype)\n",
    "        \n",
    "        score = self.metric(logits.argmax(1), target)\n",
    "        logs = {'train_loss': loss, f'train_accuracy': score}\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'log': logs,\n",
    "            'progress_bar': logs,\n",
    "            'logits': logits,\n",
    "            'target': target,\n",
    "            f'train_accuracy': score,\n",
    "        }\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        y_true = torch.cat([x['target'] for x in outputs])\n",
    "        y_pred = torch.cat([x['logits'] for x in outputs])\n",
    "        score = self.metric(y_pred.argmax(1), y_true)\n",
    "        \n",
    "        logs = {'train_loss': avg_loss, 'train_accuracy': score}\n",
    "        return {'log': logs, 'progress_bar': logs}\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: torch.Tensor, batch_idx: int\n",
    "    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n",
    "        image = batch['image']\n",
    "        target = batch['target']\n",
    "        logits, loss = self(image, target)\n",
    "        score = self.metric(logits.argmax(1), target)\n",
    "        logs = {'valid_loss': loss, f'valid_accuracy': score}\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'log': logs,\n",
    "            'progress_bar': logs,\n",
    "            'logits': logits,\n",
    "            'target': target,\n",
    "            f'valid_accuracy': score,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        y_true = torch.cat([x['target'] for x in outputs])\n",
    "        y_pred = torch.cat([x['logits'] for x in outputs])\n",
    "        score = self.metric(y_pred.argmax(1), y_true)\n",
    "\n",
    "        # score = torch.tensor(1.0, device=self.device)\n",
    "        logs = {'valid_loss': avg_loss, f'valid_accuracy': score, 'accuracy': score}\n",
    "        return {'valid_loss': avg_loss, 'log': logs, 'progress_bar': logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "\n",
    "- Schedulers\n",
    ">- add reduce LR on plateau  - Done\n",
    ">- cosine annealing LR\n",
    ">- Warm restarts\n",
    "\n",
    "- snapshot ensemble\n",
    "- Weights\n",
    ">- Noisy student from TIMM ---- vvvv imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedModels = {0:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_0.ckpt',\n",
    "#               1:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_1.ckpt',\n",
    "#               2:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_2.ckpt',\n",
    "#               3:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_3.ckpt',\n",
    "#               4:r'C:\\Users\\Kaggle\\Leaf_Classification\\saved_models\\Image 576\\tf_efficientnet_b5_ns_sz_576_timm_finetune_TemperedLoss_10Epochs\\avgWeights\\_Avg_fold_4.ckpt'\n",
    "#               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#time.sleep(60*5)\n",
    "for f in range(5):\n",
    "    print(f\"Running Fold:{f}\")\n",
    "    model = Net()\n",
    "    dm = CassavaDataModule(labels, train_augs, valid_augs, f'{base_dir}/train_images/',bs=12,fold=f)\n",
    "\n",
    "    modelSavePath = f'C:/Users/Kaggle/Leaf_Classification/saved_models/{f}'\n",
    "    if not os.path.exists(modelSavePath):\n",
    "        os.makedirs(modelSavePath)\n",
    "        \n",
    "    trainer = pl.Trainer(\n",
    "        auto_scale_batch_size = False,\n",
    "        checkpoint_callback=ModelCheckpoint(monitor='valid_accuracy',\n",
    "                                            save_top_k=2, dirpath = modelSavePath,filename='{epoch}_{valid_loss:.4f}_{valid_accuracy:.4f}', mode='max'),\n",
    "        accumulate_grad_batches = 1,\n",
    "        gpus=1,\n",
    "        max_epochs=25,\n",
    "        num_sanity_val_steps=0,\n",
    "        weights_summary='top',\n",
    "       precision = 16,\n",
    "        #amp_backend = 'apex',\n",
    "        callbacks = [EarlyStopping(monitor='valid_accuracy', patience=4, mode='max')]\n",
    "    )\n",
    "    \n",
    "    lit_model = LitCassava(model)\n",
    "    \n",
    "#     checkpoint = torch.load(savedModels[f], map_location=lambda storage, loc: storage)\n",
    "#    checkpoint['state_dict']['model.loss.nll_loss.weight'] = torch.tensor(alpha)\n",
    "#     lit_model.load_state_dict(checkpoint,strict=True)\n",
    "\n",
    "    trainer.fit(lit_model, dm)\n",
    "\n",
    "    del trainer,model,lit_model\n",
    "    gc.collect()\n",
    "    time.sleep(60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
